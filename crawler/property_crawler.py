"""
Enhanced Property Crawler - Class chÃ­nh
"""

import asyncio
from datetime import datetime
from typing import Dict, List, Any
from .property_extractor import PropertyExtractor
from .utils import FileUtils


class EnhancedPropertyCrawler:
    def __init__(self):
        self.extractor = PropertyExtractor()
    
    async def _crawl_single_property(self, url: str) -> Dict[str, Any]:
        """
        Private method Ä‘á»ƒ crawl má»™t property (chá»‰ dÃ¹ng ná»™i bá»™ cho crawl_multiple_properties)
        """
        print(f"ğŸš€ Crawling: {url}")
        
        # Extract dá»¯ liá»‡u báº±ng crawl4ai
        result = await self.extractor.extract_property_data(url)
        
        if result['success']:
            # Validate vÃ  táº¡o PropertyModel
            property_model = self.extractor.validate_and_create_property_model(
                result['property_data']
            )
            
            # Convert vá» dict Ä‘á»ƒ serialize JSON
            result['property_data'] = property_model.dict(exclude_none=True)
            
            print(f"âœ… Success: Extracted {len(result['property_data'])} fields")
        else:
            print(f"âŒ Failed: {result.get('error', 'Unknown error')}")
        
        return result

    async def crawl_multiple_properties(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        Crawl nhiá»u properties cÃ¹ng lÃºc
        """
        print(f"ğŸ˜ï¸ Crawling {len(urls)} properties...")
        
        # Táº¡o tasks cho táº¥t cáº£ URLs
        tasks = [self._crawl_single_property(url) for url in urls]
        
        # Cháº¡y parallel vá»›i error handling
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Xá»­ lÃ½ exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ Error crawling {urls[i]}: {result}")
                processed_results.append({
                    'success': False,
                    'url': urls[i],
                    'error': str(result),
                    'crawl_timestamp': datetime.now().isoformat()
                })
            else:
                processed_results.append(result)
        
        # Thá»‘ng kÃª
        success_count = sum(1 for r in processed_results if r.get('success', False))
        print(f"ğŸ“Š Results: {success_count}/{len(urls)} successful")
        
        return processed_results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], filename: str = None) -> str:
        """
        LÆ°u káº¿t quáº£ crawl vÃ o file JSON
        """
        return FileUtils.save_json_results(results, filename)
    
    def save_results_to_csv(self, results: List[Dict[str, Any]], filename: str = None) -> str:
        """
        LÆ°u káº¿t quáº£ crawl vÃ o file CSV
        """
        return FileUtils.save_csv_results(results, filename)


# Convenience functions
async def crawl_property_list(urls: List[str]) -> List[Dict[str, Any]]:
    """
    Convenience function Ä‘á»ƒ crawl nhiá»u properties
    """
    crawler = EnhancedPropertyCrawler()
    return await crawler.crawl_multiple_properties(urls)