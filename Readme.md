# Craw Data By AI

DÃ¹ng AI crawl dá»¯ liá»‡u tá»« trang web báº¥t Ä‘á»™ng sáº£n cá»§a Nháº­t Báº£n báº¥t ká»³ vÃ  phÃ¢n tÃ­ch theo cáº¥u trÃºc Ä‘á»‹nh nghÄ©a.

## ğŸš€ TÃ­nh nÄƒng chÃ­nh

- **AI-Powered Extraction**: Sá»­ dá»¥ng AI cá»§a crawl4ai Ä‘á»ƒ extract dá»¯ liá»‡u thÃ´ng minh
- **Structured Data**: Output JSON vá»›i 200+ fields theo PropertyModel
- **Multi-language Support**: Há»— trá»£ tiáº¿ng Nháº­t, Anh, Viá»‡t
- **Smart Image Classification**: Tá»± Ä‘á»™ng phÃ¢n loáº¡i hÃ¬nh áº£nh (exterior, interior, kitchen, etc.)
- **Transportation Info**: Extract thÃ´ng tin 3 ga tÃ u gáº§n nháº¥t
- **Comprehensive Amenities**: Detect 50+ tiá»‡n nghi khÃ¡c nhau
- **Batch Processing**: Crawl nhiá»u properties cÃ¹ng lÃºc
- **Statistics & Reports**: Táº¡o bÃ¡o cÃ¡o thá»‘ng kÃª tá»± Ä‘á»™ng

## ğŸ“ Files

```
â”œâ”€â”€ index.py              # File gá»‘c Ä‘Æ¡n giáº£n
â”œâ”€â”€ models.py            # Pydantic models (200+ fields)
â”œâ”€â”€ enhanced_crawler.py  # AI-powered crawler chÃ­nh
â”œâ”€â”€ demo.py             # Demo script vá»›i UI Ä‘áº¹p
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ README.md          # HÆ°á»›ng dáº«n nÃ y
```

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. Táº¡o mÃ´i trÆ°á»ng Python 3.10
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# hoáº·c
source venv/bin/activate  # Linux/Mac
```

### 2. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 3. Thiáº¿t láº­p crawl4ai
```bash
crawl4ai-setup
```

### 4. Kiá»ƒm tra thiáº¿t láº­p
```bash
crawl4ai-doctor
```

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### 1. Demo nhanh
```bash
python demo.py
```

### 2. Sá»­ dá»¥ng file gá»‘c (Ä‘Æ¡n giáº£n)
```bash
python index.py
```

### 3. Sá»­ dá»¥ng enhanced crawler (AI)
```bash
python enhanced_crawler.py
```

### 4. Sá»­ dá»¥ng trong code

```python
from enhanced_crawler import EnhancedPropertyCrawler
import asyncio

async def crawl_property():
    crawler = EnhancedPropertyCrawler()
    
    # Crawl má»™t property
    result = await crawler.crawl_property('https://example.com/property/123')
    
    if result['success']:
        data = result['property_data']
        print(f"TÃªn tÃ²a nhÃ : {data.get('building_name_ja')}")
        print(f"Tiá»n thuÃª: {data.get('monthly_rent')} yen")
        print(f"Diá»‡n tÃ­ch: {data.get('size')} mÂ²")
    
    # LÆ°u vÃ o file JSON
    filename = crawler.save_results_to_file([result])
    print(f"ÄÃ£ lÆ°u: {filename}")

asyncio.run(crawl_property())
```

## ğŸ—ï¸ PropertyModel Structure

PropertyModel bao gá»“m 200+ fields:

### ThÃ´ng tin cÆ¡ báº£n
- `property_csv_id`: MÃ£ Ä‘á»‹nh danh
- `link`: URL gá»‘c  
- `building_name_ja/en/vi`: TÃªn tÃ²a nhÃ  Ä‘a ngÃ´n ngá»¯
- `building_type`: Loáº¡i tÃ²a nhÃ 

### Äá»‹a chá»‰
- `prefecture`: Tá»‰nh (æ±äº¬éƒ½, å¤§é˜ªåºœ, etc.)
- `city`: ThÃ nh phá»‘
- `district`: Quáº­n/phÆ°á»ng
- `chome_banchi`: Sá»‘ nhÃ  theo há»‡ thá»‘ng Nháº­t

### ThÃ´ng tin phÃ²ng
- `room_type`: Loáº¡i phÃ²ng (1K, 1DK, 2LDK, etc.)
- `size`: Diá»‡n tÃ­ch (mÂ²)
- `floor_no`: Sá»‘ táº§ng
- `monthly_rent`: Tiá»n thuÃª/thÃ¡ng
- `monthly_maintenance`: PhÃ­ quáº£n lÃ½

### Giao thÃ´ng (3 ga gáº§n nháº¥t)
- `station_name_1/2/3`: TÃªn ga
- `train_line_name_1/2/3`: TÃªn tuyáº¿n
- `walk_1/2/3`: Thá»i gian Ä‘i bá»™ (phÃºt)

### Tiá»‡n nghi (50+ items)
- `aircon`: Äiá»u hÃ²a (Y/N)
- `elevator`: Thang mÃ¡y (Y/N)
- `autolock`: KhÃ³a tá»± Ä‘á»™ng (Y/N)
- `parking`: Chá»— Ä‘áº­u xe (Y/N)
- `internet_wifi`: WiFi (Y/N)
- ... vÃ  nhiá»u tiá»‡n nghi khÃ¡c

### HÃ¬nh áº£nh
- `images`: Array vá»›i category vÃ  URL

### Tá»a Ä‘á»™
- `map_lat`: VÄ© Ä‘á»™
- `map_lng`: Kinh Ä‘á»™

## ğŸ“Š Sample Output

```json
{
  "success": true,
  "url": "https://example.com/property/123",
  "property_data": {
    "property_csv_id": "PROP_123",
    "building_name_ja": "ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ³ã‚·ãƒ§ãƒ³",
    "building_type": "Apartment",
    "prefecture": "æ±äº¬éƒ½",
    "city": "æ¸‹è°·åŒº",
    "room_type": "1K",
    "size": 25.5,
    "monthly_rent": 120000,
    "station_name_1": "æ¸‹è°·é§…",
    "train_line_name_1": "JRå±±æ‰‹ç·š",
    "walk_1": 5,
    "aircon": "Y",
    "elevator": "Y",
    "images": [
      {
        "category": "exterior",
        "url": "https://example.com/image1.jpg"
      }
    ],
    "map_lat": 35.6762,
    "map_lng": 139.6503
  }
}
```

## ğŸ¤– AI Configuration

Crawler sá»­ dá»¥ng AI Ä‘á»ƒ extract dá»¯ liá»‡u thÃ´ng minh:

- **Provider**: Ollama/llama3.2 (local) hoáº·c OpenAI/Claude
- **Schema-based**: Extract theo cáº¥u trÃºc PropertyModel
- **Smart Conversion**: Tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i ä¸‡å†† â†’ yen, åª â†’ mÂ²
- **Context-aware**: Hiá»ƒu ngá»¯ cáº£nh tiáº¿ng Nháº­t

### Thay Ä‘á»•i AI Provider

Trong `enhanced_crawler.py`:

```python
# Sá»­ dá»¥ng OpenAI
extraction_strategy = LLMExtractionStrategy(
    provider="openai/gpt-4",
    api_token="your-api-key",
    # ...
)

# Sá»­ dá»¥ng Claude
extraction_strategy = LLMExtractionStrategy(
    provider="anthropic/claude-3",
    api_token="your-api-key", 
    # ...
)
```

## ğŸ“ˆ Statistics & Reports

Tá»± Ä‘á»™ng táº¡o bÃ¡o cÃ¡o thá»‘ng kÃª:

```json
{
  "summary": {
    "total_properties": 100,
    "successful_crawls": 95,
    "success_rate": "95.0%"
  },
  "statistics": {
    "building_types": {"Apartment": 80, "House": 15},
    "prefectures": {"æ±äº¬éƒ½": 60, "å¤§é˜ªåºœ": 25},
    "price_ranges": {"50k_100k": 40, "100k_200k": 35},
    "amenities": {"aircon": 90, "elevator": 75}
  }
}
```

## ğŸ”§ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **"Failed to extract content"**
   - Kiá»ƒm tra crawl4ai-setup
   - Kiá»ƒm tra internet connection
   - Thá»­ URL khÃ¡c

2. **"AI provider error"**
   - Kiá»ƒm tra Ollama Ä‘Ã£ cháº¡y: `ollama serve`
   - Hoáº·c thay Ä‘á»•i provider trong code

3. **"Timeout error"**
   - TÄƒng page_timeout trong config
   - Kiá»ƒm tra tá»‘c Ä‘á»™ máº¡ng

### Debug mode:

```python
# ThÃªm vÃ o enhanced_crawler.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸš¨ LÆ°u Ã½ quan trá»ng

1. **Rate Limiting**: ThÃªm delay giá»¯a requests
2. **Legal Compliance**: TuÃ¢n thá»§ robots.txt vÃ  ToS
3. **Resource Usage**: AI extraction tá»‘n tÃ i nguyÃªn
4. **Data Accuracy**: LuÃ´n verify dá»¯ liá»‡u quan trá»ng

## ğŸ¯ Roadmap

- [ ] Há»— trá»£ thÃªm website báº¥t Ä‘á»™ng sáº£n
- [ ] Cáº£i thiá»‡n accuracy cá»§a AI extraction  
- [ ] ThÃªm export CSV/Excel
- [ ] Dashboard web interface
- [ ] Real-time monitoring
- [ ] Multi-threading optimization

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Test thoroughly
4. Submit pull request

---

**Happy Crawling with AI! ğŸ¤–ğŸ **