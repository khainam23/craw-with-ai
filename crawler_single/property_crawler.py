"""
Enhanced Property Crawler - Class ch√≠nh
"""

import asyncio
from datetime import datetime
from typing import Dict, List, Any
from .property_extractor import PropertyExtractor
from .utils import FileUtils


class EnhancedPropertyCrawler:
    def __init__(self):
        self.extractor = PropertyExtractor()

    async def _crawl_single_property(self, url: str, verbose: bool = True) -> Dict[str, Any]:
        """
        Private method ƒë·ªÉ crawl m·ªôt property
        
        Args:
            url: URL to crawl
            verbose: Whether to print progress messages (default: True)
        """
        if verbose:
            print(f"üöÄ Crawling: {url}")
        
        try:
            # Extract d·ªØ li·ªáu b·∫±ng crawl4ai
            result = await self.extractor.extract_property_data(url)
            
            if result['success']:
                # Validate v√† t·∫°o PropertyModel
                property_model = self.extractor.validate_and_create_property_model(
                    result['property_data']
                )
                
                # Convert v·ªÅ dict ƒë·ªÉ serialize JSON
                result['property_data'] = property_model.dict(exclude_none=True)
                
                if verbose:
                    print(f"‚úÖ Success: Extracted {len(result['property_data'])} fields")
            else:
                if verbose:
                    print(f"‚ùå Failed: {result.get('error', 'Unknown error')}")
            
            return result
            
        except Exception as e:
            error_result = {
                'success': False,
                'url': url,
                'error': str(e),
                'crawl_timestamp': datetime.now().isoformat()
            }
            if verbose:
                print(f"‚ùå Exception crawling {url}: {e}")
            return error_result

    async def crawl_multiple_properties(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        Crawl nhi·ªÅu properties c√πng l√∫c
        """
        print(f"üèòÔ∏è Crawling {len(urls)} properties...")
        
        # T·∫°o tasks cho t·∫•t c·∫£ URLs
        tasks = [self._crawl_single_property(url) for url in urls]
        
        # Ch·∫°y parallel v·ªõi error handling
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # X·ª≠ l√Ω exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"‚ùå Error crawling {urls[i]}: {result}")
                processed_results.append({
                    'success': False,
                    'url': urls[i],
                    'error': str(result),
                    'crawl_timestamp': datetime.now().isoformat()
                })
            else:
                processed_results.append(result)
        
        # Th·ªëng k√™
        success_count = sum(1 for r in processed_results if r.get('success', False))
        print(f"üìä Results: {success_count}/{len(urls)} successful")
        
        return processed_results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], filename: str = None) -> str:
        """
        L∆∞u k·∫øt qu·∫£ crawl v√†o file JSON
        """
        return FileUtils.save_json_results(results, filename)