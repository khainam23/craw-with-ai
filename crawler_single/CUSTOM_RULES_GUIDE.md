# ğŸ› ï¸ Custom Rules System - HÆ°á»›ng dáº«n sá»­ dá»¥ng

## ğŸ“‹ Tá»•ng quan

Custom Rules System cho phÃ©p báº¡n tá»± viáº¿t logic extraction mÃ  khÃ´ng cáº§n sá»­a code core. Há»‡ thá»‘ng bao gá»“m:

- **Rules**: Logic Ä‘á»ƒ extract dá»¯ liá»‡u tá»« HTML
- **Hooks**: Xá»­ lÃ½ trÆ°á»›c vÃ  sau khi extract
- **Priority**: Thá»© tá»± Æ°u tiÃªn cá»§a rules

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. File chÃ­nh Ä‘á»ƒ chá»‰nh sá»­a

```
crawler_single/custom_config.py
```

Táº¥t cáº£ custom logic cá»§a báº¡n sáº½ Ä‘Æ°á»£c viáº¿t trong file nÃ y.

### 2. Cáº¥u trÃºc cÆ¡ báº£n

```python
def setup_custom_extractor() -> CustomExtractor:
    extractor = CustomExtractor()
    
    # ThÃªm rules
    extractor.add_rule(your_rule)
    
    # ThÃªm hooks
    extractor.add_pre_hook(your_pre_hook)
    extractor.add_post_hook(your_post_hook)
    
    return extractor
```

## ğŸ“ CÃ¡c loáº¡i Rules

### 1. CSS Selector Rule

```python
# Extract báº±ng CSS selector
extractor.add_rule(RuleBuilder.css_selector_rule(
    name="rent_price",           # TÃªn rule
    field="monthly_rent",        # Field trong data model
    selector=".price-value",     # CSS selector
    attribute="text",            # "text" hoáº·c attribute name
    priority=10                  # Sá»‘ cao = cháº¡y trÆ°á»›c
))
```

### 2. Regex Rule

```python
# Extract báº±ng regex
extractor.add_rule(RuleBuilder.regex_rule(
    name="rent_regex",
    field="monthly_rent", 
    pattern=r'(\d+(?:,\d+)?)ä¸‡å††',  # Regex pattern vá»›i group
    group=1,                        # Group number Ä‘á»ƒ láº¥y
    priority=5
))
```

### 3. URL Condition Rule

```python
# Rule chá»‰ Ã¡p dá»¥ng cho URL cá»¥ thá»ƒ
def extract_special_data(html: str, data: Dict[str, Any]) -> str:
    # Logic extract cá»§a báº¡n
    return "extracted_value"

extractor.add_rule(RuleBuilder.url_condition_rule(
    name="special_site",
    field="special_field",
    url_pattern=r'special-site\.com',  # Regex cho URL
    extract_func=extract_special_data,
    priority=15
))
```

### 4. Custom Rule (Linh hoáº¡t nháº¥t)

```python
def my_condition(html: str, data: Dict[str, Any]) -> bool:
    # Return True náº¿u rule cÃ³ thá»ƒ Ã¡p dá»¥ng
    return "keyword" in html

def my_action(html: str, data: Dict[str, Any]) -> str:
    # Logic extract cá»§a báº¡n
    soup = BeautifulSoup(html, 'html.parser')
    element = soup.find('div', class_='my-class')
    return element.text if element else None

rule = ExtractionRule(
    name="my_custom_rule",
    field="my_field", 
    condition=my_condition,
    action=my_action,
    priority=8
)
extractor.add_rule(rule)
```

## ğŸ”§ Hooks

### Pre-hook (Cháº¡y trÆ°á»›c extraction)

```python
def my_pre_hook(html: str, data: Dict[str, Any]) -> tuple:
    # Modify HTML hoáº·c data trÆ°á»›c khi extract
    print("ğŸ”„ Pre-processing...")
    
    # CÃ³ thá»ƒ clean HTML, thÃªm data, etc.
    cleaned_html = html.replace('unwanted', '')
    data['preprocessing_done'] = True
    
    return cleaned_html, data

extractor.add_pre_hook(my_pre_hook)
```

### Post-hook (Cháº¡y sau extraction)

```python
def my_post_hook(data: Dict[str, Any]) -> Dict[str, Any]:
    # Transform data sau khi extract
    print("ğŸ”§ Post-processing...")
    
    # TÃ­nh toÃ¡n thÃªm
    if 'monthly_rent' in data and data['monthly_rent']:
        rent = float(str(data['monthly_rent']).replace(',', ''))
        data['yearly_rent'] = str(int(rent * 12))
        data['total_cost'] = str(int(rent * 1.1))  # +10% phÃ­
    
    return data

extractor.add_post_hook(my_post_hook)
```

## ğŸ—ºï¸ VÃ­ dá»¥ thá»±c táº¿: Coordinate Conversion

Há»‡ thá»‘ng hiá»‡n táº¡i Ä‘Ã£ cÃ³ sáºµn tÃ­nh nÄƒng convert tá»a Ä‘á»™ X,Y sang lat/lng:

```python
def convert_coordinates(data: Dict[str, Any]) -> Dict[str, Any]:
    html = data.get('_html', '')
    
    # Extract MAP_X vÃ  MAP_Y tá»« hidden inputs
    x_match = re.search(r'name="[^"]*MAP_X"[^>]*value="([^"]*)"', html, re.IGNORECASE)
    y_match = re.search(r'name="[^"]*MAP_Y"[^>]*value="([^"]*)"', html, re.IGNORECASE)
    
    if x_match and y_match:
        try:
            from utils.convert_gis import xy_to_latlon_tokyo
            
            x = float(x_match.group(1))
            y = float(y_match.group(1))
            
            # Convert tá»a Ä‘á»™
            lat, lon = xy_to_latlon_tokyo(x, y, zone=9)
            
            data['map_lat'] = str(lat)
            data['map_lng'] = str(lon)
            
            print(f"ğŸ—ºï¸ Converted: X={x}, Y={y} â†’ Lat={lat:.6f}, Lng={lon:.6f}")
            
        except Exception as e:
            print(f"âŒ Coordinate conversion error: {e}")
    
    return data
```

## ğŸ¯ VÃ­ dá»¥ hoÃ n chá»‰nh

```python
def setup_custom_extractor() -> CustomExtractor:
    extractor = CustomExtractor()
    
    # 1. Extract giÃ¡ thuÃª báº±ng CSS (Æ°u tiÃªn cao)
    extractor.add_rule(RuleBuilder.css_selector_rule(
        name="rent_css",
        field="monthly_rent",
        selector=".price-display, .rent-value",
        priority=10
    ))
    
    # 2. Fallback regex náº¿u CSS khÃ´ng tÃ¬m tháº¥y
    extractor.add_rule(RuleBuilder.regex_rule(
        name="rent_regex", 
        field="monthly_rent",
        pattern=r'(\d+(?:,\d+)?)ä¸‡å††',
        group=1,
        priority=5
    ))
    
    # 3. Extract diá»‡n tÃ­ch cho site Ä‘áº·c biá»‡t
    def extract_area_special(html: str, data: Dict[str, Any]) -> str:
        match = re.search(r'é¢ç©ï¼š(\d+\.?\d*)ã¡', html)
        return match.group(1) if match else None
    
    extractor.add_rule(RuleBuilder.url_condition_rule(
        name="area_special_site",
        field="size",
        url_pattern=r'special-site\.com',
        extract_func=extract_area_special,
        priority=8
    ))
    
    # 4. Pre-hook Ä‘á»ƒ clean HTML
    def clean_html(html: str, data: Dict[str, Any]) -> tuple:
        # Remove ads, scripts, etc.
        cleaned = re.sub(r'<script.*?</script>', '', html, flags=re.DOTALL)
        cleaned = re.sub(r'<style.*?</style>', '', cleaned, flags=re.DOTALL)
        return cleaned, data
    
    # 5. Post-hook Ä‘á»ƒ tÃ­nh toÃ¡n thÃªm
    def calculate_extras(data: Dict[str, Any]) -> Dict[str, Any]:
        # TÃ­nh chi phÃ­ hÃ ng nÄƒm
        if 'monthly_rent' in data and data['monthly_rent']:
            try:
                rent = float(str(data['monthly_rent']).replace(',', ''))
                data['yearly_rent'] = str(int(rent * 12))
                data['total_cost_with_fees'] = str(int(rent * 1.15))  # +15% phÃ­
            except:
                pass
        
        # Format láº¡i sá»‘ Ä‘iá»‡n thoáº¡i
        if 'contact_phone' in data and data['contact_phone']:
            phone = re.sub(r'[^\d]', '', data['contact_phone'])
            if len(phone) >= 10:
                data['contact_phone'] = f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
        
        return data
    
    extractor.add_pre_hook(clean_html)
    extractor.add_post_hook(calculate_extras)
    
    return extractor
```

## ğŸ” Debug vÃ  Test

### 1. Xem log khi cháº¡y

```bash
cd d:\Learn\craw-data-by-ai
python -m crawler_single.main
```

Báº¡n sáº½ tháº¥y:
```
âœ… Applied rule 'rent_css' for field 'monthly_rent': 125000
ğŸ—ºï¸ Converted: X=502813.183, Y=128669.157 â†’ Lat=35.737000, Lng=139.654000
ğŸ”§ Post-processing...
```

### 2. Test rule riÃªng láº»

```python
# Trong custom_config.py, thÃªm debug
def debug_rule(html: str, data: Dict[str, Any]) -> Dict[str, Any]:
    print(f"ğŸ› Current data: {list(data.keys())}")
    print(f"ğŸ› HTML length: {len(html)}")
    return data

extractor.add_post_hook(debug_rule)
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

### 1. Priority Rules
- **Sá»‘ cao = cháº¡y trÆ°á»›c**: Priority 10 cháº¡y trÆ°á»›c Priority 5
- **First match wins**: Rule Ä‘áº§u tiÃªn thÃ nh cÃ´ng sáº½ dá»«ng, khÃ´ng thá»­ rule khÃ¡c cho cÃ¹ng field

### 2. Error Handling
- Rules tá»± Ä‘á»™ng catch exception, khÃ´ng crash há»‡ thá»‘ng
- Náº¿u rule fail, sáº½ thá»­ rule tiáº¿p theo (priority tháº¥p hÆ¡n)

### 3. Performance
- Äá»«ng viáº¿t regex quÃ¡ phá»©c táº¡p
- Sá»­ dá»¥ng CSS selector khi cÃ³ thá»ƒ (nhanh hÆ¡n regex)
- Pre-hook cÃ³ thá»ƒ clean HTML Ä‘á»ƒ tÄƒng tá»‘c

### 4. Data Fields
- Sá»­ dá»¥ng Ä‘Ãºng tÃªn field theo PropertyModel
- CÃ³ thá»ƒ táº¡o field má»›i, nhÆ°ng cáº§n update model náº¿u muá»‘n lÆ°u DB

## ğŸ¨ Tips vÃ  Tricks

### 1. Multi-step Extraction

```python
def extract_complex_data(html: str, data: Dict[str, Any]) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    
    # Step 1: Find container
    container = soup.find('div', class_='property-info')
    if not container:
        return None
    
    # Step 2: Extract from container
    price_elem = container.find('span', class_='price')
    return price_elem.text.strip() if price_elem else None
```

### 2. Conditional Logic

```python
def smart_extraction(html: str, data: Dict[str, Any]) -> str:
    # Thá»­ nhiá»u cÃ¡ch khÃ¡c nhau
    methods = [
        lambda: re.search(r'method1_pattern', html),
        lambda: re.search(r'method2_pattern', html),
        lambda: BeautifulSoup(html, 'html.parser').find('div', class_='fallback')
    ]
    
    for method in methods:
        try:
            result = method()
            if result:
                return result.group(1) if hasattr(result, 'group') else result.text
        except:
            continue
    
    return None
```

### 3. Data Validation

```python
def validate_and_clean(data: Dict[str, Any]) -> Dict[str, Any]:
    # Validate giÃ¡ thuÃª
    if 'monthly_rent' in data:
        try:
            rent = float(str(data['monthly_rent']).replace(',', ''))
            if rent < 10000 or rent > 1000000:  # Reasonable range
                print(f"âš ï¸ Suspicious rent value: {rent}")
                del data['monthly_rent']
        except:
            del data['monthly_rent']
    
    # Clean phone number
    if 'contact_phone' in data:
        phone = re.sub(r'[^\d-]', '', data['contact_phone'])
        data['contact_phone'] = phone if len(phone) >= 10 else None
    
    return data

extractor.add_post_hook(validate_and_clean)
```

---

## ğŸš€ Báº¯t Ä‘áº§u ngay

1. Má»Ÿ file `crawler_single/custom_config.py`
2. Uncomment vÃ  modify cÃ¡c example rules
3. Cháº¡y crawler Ä‘á»ƒ test
4. Xem log vÃ  adjust theo nhu cáº§u

Happy crawling! ğŸ•·ï¸âœ¨