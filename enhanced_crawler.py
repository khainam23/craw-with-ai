"""
Enhanced Property Crawler sá»­ dá»¥ng crawl4ai (khÃ´ng cáº§n LLM)
Táº­n dá»¥ng sá»©c máº¡nh cá»§a crawl4ai Ä‘á»ƒ crawl dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from models import PropertyModel, PropertyImage
import time as t


class PropertyExtractor:
    """
    Sá»­ dá»¥ng crawl4ai Ä‘á»ƒ extract dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n (khÃ´ng cáº§n LLM)
    """
    
    def __init__(self):
        self.browser_config = BrowserConfig(
            headless=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
    
    async def extract_property_data(self, url: str) -> Dict[str, Any]:
        """
        Extract dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« URL vá»›i Ä‘áº§y Ä‘á»§ thÃ´ng tin theo PropertyModel
        """
        # Cáº¥u hÃ¬nh crawler Ä‘á»ƒ láº¥y Ä‘áº§y Ä‘á»§ thÃ´ng tin
        run_config = CrawlerRunConfig(
            wait_for_images=True,
            delay_before_return_html=3.0,
            page_timeout=45000,
            remove_overlay_elements=True,
            screenshot=True
        )
        
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                result = await crawler.arun(
                    url=url,
                    config=run_config
                )
                
                if result.success:
                    # Extract comprehensive property data
                    extracted_data = self._extract_comprehensive_data(url, result)
                    
                    print(f"âœ… Successfully crawled: {url}")
                    print(f"ğŸ” Title: {extracted_data.get('building_name_ja', 'N/A')}")
                    print(f"ğŸ” Extracted {len([k for k, v in extracted_data.items() if v is not None])} fields with data")
                    
                    return {
                        'success': True,
                        'url': url,
                        'property_data': extracted_data,
                        'crawl_timestamp': datetime.now().isoformat(),
                        'raw_html_length': len(result.html) if result.html else 0,
                        'metadata': {
                            'title': result.metadata.get('title', ''),
                            'description': result.metadata.get('description', ''),
                            'keywords': result.metadata.get('keywords', '')
                        }
                    }
                else:
                    return {
                        'success': False,
                        'url': url,
                        'error': result.error_message or 'Failed to extract content',
                        'crawl_timestamp': datetime.now().isoformat()
                    }
                    
        except Exception as e:
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'crawl_timestamp': datetime.now().isoformat()
            }
    
    def _extract_comprehensive_data(self, url: str, result) -> Dict[str, Any]:
        """
        Extract comprehensive property data tá»« crawl result
        """
        # Khá»Ÿi táº¡o data structure vá»›i táº¥t cáº£ fields tá»« PropertyModel
        extracted_data = {
            # ThÃ´ng tin cÆ¡ báº£n
            'link': url,
            'property_csv_id': self._generate_property_id(url),
            'create_date': datetime.now().isoformat(),
            
            # Äá»‹a chá»‰
            'postcode': None,
            'prefecture': None,
            'city': None,
            'district': None,
            'chome_banchi': None,
            
            # ThÃ´ng tin tÃ²a nhÃ 
            'building_type': None,
            'year': None,
            
            # TÃªn tÃ²a nhÃ  Ä‘a ngÃ´n ngá»¯
            'building_name_en': None,
            'building_name_ja': None,
            'building_name_vi': None,
            
            # MÃ´ táº£ tÃ²a nhÃ  Ä‘a ngÃ´n ngá»¯
            'building_description_en': None,
            'building_description_ja': None,
            'building_description_vi': None,
            
            # Äá»‹a danh gáº§n Ä‘Ã³ Ä‘a ngÃ´n ngá»¯
            'building_landmarks_en': None,
            'building_landmarks_ja': None,
            'building_landmarks_vi': None,
            
            # ThÃ´ng tin ga tÃ u (5 ga)
            'station_name_1': None, 'train_line_name_1': None, 'walk_1': None, 'bus_1': None, 'car_1': None, 'cycle_1': None,
            'station_name_2': None, 'train_line_name_2': None, 'walk_2': None, 'bus_2': None, 'car_2': None, 'cycle_2': None,
            'station_name_3': None, 'train_line_name_3': None, 'walk_3': None, 'bus_3': None, 'car_3': None, 'cycle_3': None,
            'station_name_4': None, 'train_line_name_4': None, 'walk_4': None, 'bus_4': None, 'car_4': None, 'cycle_4': None,
            'station_name_5': None, 'train_line_name_5': None, 'walk_5': None, 'bus_5': None, 'car_5': None, 'cycle_5': None,
            
            # Tá»a Ä‘á»™ Ä‘á»‹a lÃ½
            'map_lat': None,
            'map_lng': None,
            
            # ThÃ´ng tin cáº¥u trÃºc tÃ²a nhÃ 
            'num_units': None,
            'floors': None,
            'basement_floors': None,
            
            # ThÃ´ng tin Ä‘áº­u xe
            'parking': None,
            'parking_cost': None,
            'bicycle_parking': None,
            'motorcycle_parking': None,
            
            # ThÃ´ng tin cáº¥u trÃºc vÃ  phong cÃ¡ch
            'structure': None,
            'building_notes': None,
            'building_style': None,
            
            # Tiá»‡n Ã­ch tÃ²a nhÃ 
            'autolock': None, 'credit_card': None, 'concierge': None, 'delivery_box': None,
            'elevator': None, 'gym': None, 'newly_built': None, 'pets': None,
            'swimming_pool': None, 'ur': None,
            
            # ThÃ´ng tin cÄƒn há»™
            'room_type': None,
            'size': None,
            'unit_no': None,
            'ad_type': None,
            'available_from': None,
            
            # MÃ´ táº£ báº¥t Ä‘á»™ng sáº£n Ä‘a ngÃ´n ngá»¯
            'property_description_en': None,
            'property_description_ja': None,
            'property_description_vi': None,
            
            # Chi phÃ­ khÃ¡c Ä‘a ngÃ´n ngá»¯
            'property_other_expenses_en': None,
            'property_other_expenses_ja': None,
            'property_other_expenses_vi': None,
            
            # Loáº¡i ná»•i báº­t
            'featured_a': None, 'featured_b': None, 'featured_c': None,
            
            # ThÃ´ng tin táº§ng vÃ  giÃ¡ thuÃª
            'floor_no': None,
            'monthly_rent': None,
            'monthly_maintenance': None,
            
            # CÃ¡c khoáº£n phÃ­
            'months_deposit': None, 'numeric_deposit': None,
            'months_key': None, 'numeric_key': None,
            'months_guarantor': None, 'numeric_guarantor': None,
            'months_agency': None, 'numeric_agency': None,
            'months_renewal': None, 'numeric_renewal': None,
            'months_deposit_amortization': None, 'numeric_deposit_amortization': None,
            'months_security_deposit': None, 'numeric_security_deposit': None,
            
            # CÃ¡c phÃ­ khÃ¡c
            'lock_exchange': None,
            'fire_insurance': None,
            'other_initial_fees': None,
            'other_subscription_fees': None,
            
            # ThÃ´ng tin báº£o lÃ£nh
            'no_guarantor': None,
            'guarantor_agency': None,
            'guarantor_agency_name': None,
            'numeric_guarantor_max': None,
            
            # ThÃ´ng tin thuÃª
            'rent_negotiable': None,
            'renewal_new_rent': None,
            'lease_date': None,
            'lease_months': None,
            'lease_type': None,
            'short_term_ok': None,
            
            # ThÃ´ng tin ban cÃ´ng vÃ  ghi chÃº
            'balcony_size': None,
            'property_notes': None,
            'discount': None,
            
            # HÆ°á»›ng cÄƒn há»™
            'facing_north': None, 'facing_northeast': None, 'facing_east': None, 'facing_southeast': None,
            'facing_south': None, 'facing_southwest': None, 'facing_west': None, 'facing_northwest': None,
            
            # Tiá»‡n nghi cÄƒn há»™ (ráº¥t nhiá»u)
            'aircon': None, 'aircon_heater': None, 'all_electric': None, 'auto_fill_bath': None,
            'balcony': None, 'bath': None, 'bath_water_heater': None, 'blinds': None,
            'bs': None, 'cable': None, 'carpet': None, 'cleaning_service': None,
            'counter_kitchen': None, 'dishwasher': None, 'drapes': None, 'female_only': None,
            'fireplace': None, 'flooring': None, 'full_kitchen': None, 'furnished': None,
            'gas': None, 'induction_cooker': None, 'internet_broadband': None, 'internet_wifi': None,
            'japanese_toilet': None, 'linen': None, 'loft': None, 'microwave': None,
            'oven': None, 'phoneline': None, 'range': None, 'refrigerator': None,
            'refrigerator_freezer': None, 'roof_balcony': None, 'separate_toilet': None, 'shower': None,
            'soho': None, 'storage': None, 'student_friendly': None, 'system_kitchen': None,
            'tatami': None, 'underfloor_heating': None, 'unit_bath': None, 'utensils_cutlery': None,
            'veranda': None, 'washer_dryer': None, 'washing_machine': None, 'washlet': None,
            'western_toilet': None, 'yard': None,
            
            # Media links
            'youtube': None,
            'vr_link': None,
            
            # HÃ¬nh áº£nh
            'images': []
        }
        
        # Extract data tá»« HTML content
        html_content = result.html if result.html else ""
        markdown_content = result.markdown if result.markdown else ""
        
        # Extract basic info tá»« metadata
        metadata = result.metadata or {}
        title = metadata.get('title', '')
        description = metadata.get('description', '')
        
        # Set building name tá»« title
        if title:
            extracted_data['building_name_ja'] = title
            extracted_data['building_name_en'] = title  # CÃ³ thá»ƒ translate sau
        
        # Set description
        if description:
            extracted_data['property_description_ja'] = description
            extracted_data['property_description_en'] = description  # CÃ³ thá»ƒ translate sau
        
        # Extract images tá»« HTML content
        images = self._extract_images_from_html(html_content)
        extracted_data['images'] = images
        
        # Extract structured data tá»« HTML patterns
        extracted_data = self._extract_from_html_patterns(html_content, extracted_data)
        
        # Extract tá»« markdown content
        extracted_data = self._extract_from_markdown(markdown_content, extracted_data)
        
        return extracted_data
    
    def _extract_images_from_html(self, html: str) -> List[Dict[str, str]]:
        """
        Extract images tá»« HTML content
        """
        if not html:
            return []
        
        images = []
        
        # Extract img tags with src attributes
        img_patterns = [
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*alt=["\']([^"\']*)["\'][^>]*>',
            r'<img[^>]+alt=["\']([^"\']*)["\'][^>]*src=["\']([^"\']+)["\'][^>]*>',
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        ]
        
        for pattern in img_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) == 2:
                        # Pattern with both src and alt
                        if 'src=' in pattern and pattern.index('src=') < pattern.index('alt='):
                            src, alt = match
                        else:
                            alt, src = match
                    else:
                        src = match[0]
                        alt = ""
                else:
                    src = match
                    alt = ""
                
                # Filter out small icons and invalid URLs
                if (src and 
                    not any(skip in src.lower() for skip in ['icon', 'logo', 'button', 'arrow']) and
                    (src.startswith('http') or src.startswith('/')) and
                    any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif'])):
                    
                    # Make relative URLs absolute
                    if src.startswith('/'):
                        # Extract domain from the current URL if needed
                        src = src  # Keep as is for now, can be enhanced later
                    
                    image_data = {
                        'url': src,
                        'category': self._categorize_image(alt, len(images))
                    }
                    images.append(image_data)
                    
                    # Limit to 20 images
                    if len(images) >= 20:
                        break
            
            if len(images) >= 20:
                break
        
        return images
    
    def _categorize_image(self, alt_text: str, index: int) -> str:
        """
        Categorize image dá»±a trÃªn alt text hoáº·c index
        """
        alt_lower = alt_text.lower() if alt_text else ""
        
        # Japanese keywords for categorization
        if any(word in alt_lower for word in ['exterior', 'outside', 'building', 'facade', 'å¤–è¦³', 'å»ºç‰©']):
            return 'exterior'
        elif any(word in alt_lower for word in ['interior', 'room', 'living', 'bedroom', 'å®¤å†…', 'éƒ¨å±‹', 'ãƒªãƒ“ãƒ³ã‚°']):
            return 'interior'
        elif any(word in alt_lower for word in ['kitchen', 'dining', 'ã‚­ãƒƒãƒãƒ³', 'å°æ‰€']):
            return 'kitchen'
        elif any(word in alt_lower for word in ['bathroom', 'bath', 'toilet', 'ãƒã‚¹', 'ãƒˆã‚¤ãƒ¬', 'æµ´å®¤']):
            return 'bathroom'
        elif any(word in alt_lower for word in ['balcony', 'terrace', 'veranda', 'ãƒãƒ«ã‚³ãƒ‹ãƒ¼', 'ãƒ™ãƒ©ãƒ³ãƒ€']):
            return 'balcony'
        else:
            # Default categorization based on index
            categories = ['exterior', 'interior', 'kitchen', 'bathroom', 'balcony', 'other']
            return categories[index % len(categories)]
    
    def _extract_from_html_patterns(self, html: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract data tá»« HTML patterns (regex-based)
        """
        if not html:
            return data
        
        # Extract rent price patterns
        rent_patterns = [
            r'Â¥([\d,]+)',
            r'(\d+)ä¸‡å††',
            r'(\d+),(\d+)å††'
        ]
        
        for pattern in rent_patterns:
            matches = re.findall(pattern, html)
            if matches:
                try:
                    if isinstance(matches[0], tuple):
                        # Handle comma-separated numbers
                        rent_value = ''.join(matches[0])
                    else:
                        rent_value = matches[0].replace(',', '')
                    data['monthly_rent'] = rent_value
                    break
                except:
                    continue
        
        # Extract room type patterns
        room_patterns = [
            r'(\d+[LDKS]+)',
            r'(\d+K)',
            r'(\d+DK)',
            r'(\d+LDK)'
        ]
        
        for pattern in room_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['room_type'] = matches[0]
                break
        
        # Extract size patterns
        size_patterns = [
            r'(\d+\.?\d*)ã¡',
            r'(\d+\.?\d*)mÂ²',
            r'(\d+\.?\d*)\s*å¹³ç±³'
        ]
        
        for pattern in size_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['size'] = matches[0]
                break
        
        # Extract floor patterns
        floor_patterns = [
            r'(\d+)éš',
            r'(\d+)F'
        ]
        
        for pattern in floor_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['floor_no'] = matches[0]
                break
        
        # Extract year patterns
        year_patterns = [
            r'ç¯‰(\d{4})å¹´',
            r'(\d{4})å¹´ç¯‰',
            r'å»ºç¯‰å¹´.*?(\d{4})'
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['year'] = matches[0]
                break
        
        # Extract boolean amenities (Y/N fields) - comprehensive list
        amenity_keywords = {
            # Building amenities
            'elevator': ['ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼', 'elevator', 'EV'],
            'autolock': ['ã‚ªãƒ¼ãƒˆãƒ­ãƒƒã‚¯', 'auto lock', 'autoloc'],
            'delivery_box': ['å®…é…ãƒœãƒƒã‚¯ã‚¹', 'delivery box', 'å®…é…BOX'],
            'concierge': ['ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥', 'concierge', 'ãƒ•ãƒ­ãƒ³ãƒˆ'],
            'gym': ['ã‚¸ãƒ ', 'gym', 'ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹'],
            'swimming_pool': ['ãƒ—ãƒ¼ãƒ«', 'pool', 'swimming'],
            
            # Parking
            'parking': ['é§è»Šå ´', 'parking', 'é§è»Š'],
            'bicycle_parking': ['é§è¼ªå ´', 'bicycle parking', 'è‡ªè»¢è»Š'],
            'motorcycle_parking': ['ãƒã‚¤ã‚¯ç½®å ´', 'motorcycle', 'ãƒã‚¤ã‚¯'],
            
            # Unit amenities
            'aircon': ['ã‚¨ã‚¢ã‚³ãƒ³', 'air conditioning', 'aircon', 'AC'],
            'aircon_heater': ['ã‚¨ã‚¢ã‚³ãƒ³æš–æˆ¿', 'air conditioning heater'],
            'internet_wifi': ['WiFi', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ', 'internet', 'ãƒãƒƒãƒˆ'],
            'cable': ['ã‚±ãƒ¼ãƒ–ãƒ«TV', 'cable', 'CATV'],
            'bs': ['BS', 'BSæ”¾é€', 'satellite'],
            
            # Kitchen
            'system_kitchen': ['ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒƒãƒãƒ³', 'system kitchen'],
            'counter_kitchen': ['ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚­ãƒƒãƒãƒ³', 'counter kitchen'],
            'full_kitchen': ['ãƒ•ãƒ«ã‚­ãƒƒãƒãƒ³', 'full kitchen'],
            'induction_cooker': ['IHã‚¯ãƒƒã‚­ãƒ³ã‚°', 'induction', 'IH'],
            'gas': ['ã‚¬ã‚¹', 'gas'],
            'microwave': ['é›»å­ãƒ¬ãƒ³ã‚¸', 'microwave'],
            'oven': ['ã‚ªãƒ¼ãƒ–ãƒ³', 'oven'],
            'dishwasher': ['é£Ÿæ´—æ©Ÿ', 'dishwasher', 'é£Ÿå™¨æ´—ã„'],
            'refrigerator': ['å†·è”µåº«', 'refrigerator', 'å†·è”µ'],
            'refrigerator_freezer': ['å†·å‡å†·è”µåº«', 'freezer'],
            
            # Bathroom
            'bath': ['ãƒã‚¹', 'bath', 'æµ´å®¤'],
            'separate_toilet': ['ç‹¬ç«‹æ´—é¢å°', 'separate toilet', 'ç‹¬ç«‹'],
            'unit_bath': ['ãƒ¦ãƒ‹ãƒƒãƒˆãƒã‚¹', 'unit bath'],
            'auto_fill_bath': ['è‡ªå‹•çµ¦æ¹¯', 'auto fill'],
            'shower': ['ã‚·ãƒ£ãƒ¯ãƒ¼', 'shower'],
            'japanese_toilet': ['å’Œå¼ãƒˆã‚¤ãƒ¬', 'japanese toilet'],
            'western_toilet': ['æ´‹å¼ãƒˆã‚¤ãƒ¬', 'western toilet'],
            'washlet': ['ã‚¦ã‚©ã‚·ãƒ¥ãƒ¬ãƒƒãƒˆ', 'washlet'],
            
            # Flooring & Interior
            'flooring': ['ãƒ•ãƒ­ãƒ¼ãƒªãƒ³ã‚°', 'flooring', 'ãƒ•ãƒ­ã‚¢'],
            'tatami': ['ç•³', 'tatami'],
            'carpet': ['ã‚«ãƒ¼ãƒšãƒƒãƒˆ', 'carpet'],
            'underfloor_heating': ['åºŠæš–æˆ¿', 'underfloor heating'],
            
            # Storage & Space
            'storage': ['åç´', 'storage', 'ã‚¯ãƒ­ãƒ¼ã‚¼ãƒƒãƒˆ'],
            'loft': ['ãƒ­ãƒ•ãƒˆ', 'loft'],
            'balcony': ['ãƒãƒ«ã‚³ãƒ‹ãƒ¼', 'balcony'],
            'veranda': ['ãƒ™ãƒ©ãƒ³ãƒ€', 'veranda'],
            'roof_balcony': ['ãƒ«ãƒ¼ãƒ•ãƒãƒ«ã‚³ãƒ‹ãƒ¼', 'roof balcony'],
            'yard': ['åº­', 'yard', 'ã‚¬ãƒ¼ãƒ‡ãƒ³'],
            
            # Appliances
            'washing_machine': ['æ´—æ¿¯æ©Ÿ', 'washing machine'],
            'washer_dryer': ['æ´—æ¿¯ä¹¾ç‡¥æ©Ÿ', 'washer dryer'],
            'furnished': ['å®¶å…·ä»˜ã', 'furnished', 'å®¶å…·'],
            'all_electric': ['ã‚ªãƒ¼ãƒ«é›»åŒ–', 'all electric'],
            
            # Special features
            'pets': ['ãƒšãƒƒãƒˆ', 'pet', 'ãƒšãƒƒãƒˆå¯'],
            'female_only': ['å¥³æ€§é™å®š', 'female only', 'å¥³æ€§å°‚ç”¨'],
            'student_friendly': ['å­¦ç”Ÿå¯', 'student', 'å­¦ç”Ÿ'],
            'soho': ['SOHO', 'soho', 'äº‹å‹™æ‰€å¯'],
            'newly_built': ['æ–°ç¯‰', 'newly built', 'æ–°ç¯‰ç‰©ä»¶']
        }
        
        for field, keywords in amenity_keywords.items():
            for keyword in keywords:
                if keyword in html:
                    data[field] = 'Y'
                    break
            if data[field] != 'Y':
                data[field] = None  # Keep as None if not found
        
        # Extract additional pricing information
        deposit_patterns = [
            r'æ•·é‡‘[ï¼š:]\s*(\d+)ä¸‡å††',
            r'æ•·é‡‘[ï¼š:]\s*(\d+)ãƒ¶?æœˆ',
            r'deposit[ï¼š:]\s*Â¥([\d,]+)'
        ]
        
        for pattern in deposit_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['numeric_deposit'] = matches[0].replace(',', '')
                break
        
        # Extract key money patterns
        key_patterns = [
            r'ç¤¼é‡‘[ï¼š:]\s*(\d+)ä¸‡å††',
            r'ç¤¼é‡‘[ï¼š:]\s*(\d+)ãƒ¶?æœˆ',
            r'key money[ï¼š:]\s*Â¥([\d,]+)'
        ]
        
        for pattern in key_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['numeric_key'] = matches[0].replace(',', '')
                break
        
        # Extract maintenance fee patterns
        maintenance_patterns = [
            r'ç®¡ç†è²»[ï¼š:]\s*(\d+)å††',
            r'å…±ç›Šè²»[ï¼š:]\s*(\d+)å††',
            r'maintenance[ï¼š:]\s*Â¥([\d,]+)'
        ]
        
        for pattern in maintenance_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['monthly_maintenance'] = matches[0].replace(',', '')
                break
        
        # Extract building structure
        structure_patterns = [
            r'æ§‹é€ [ï¼š:]\s*([^<\n]+)',
            r'(RC|SRC|æœ¨é€ |é‰„éª¨|è»½é‡é‰„éª¨)',
            r'structure[ï¼š:]\s*([^<\n]+)'
        ]
        
        for pattern in structure_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['structure'] = matches[0].strip()
                break
        
        # Extract building floors
        building_floor_patterns = [
            r'åœ°ä¸Š(\d+)éš',
            r'(\d+)éšå»º',
            r'building.*?(\d+)\s*floors?'
        ]
        
        for pattern in building_floor_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['floors'] = matches[0]
                break
        
        # Extract coordinates from maps or scripts
        lat_patterns = [
            r'lat["\']?\s*[:=]\s*([0-9.-]+)',
            r'latitude["\']?\s*[:=]\s*([0-9.-]+)',
            r'ç·¯åº¦[ï¼š:]\s*([0-9.-]+)'
        ]
        
        lng_patterns = [
            r'lng["\']?\s*[:=]\s*([0-9.-]+)',
            r'longitude["\']?\s*[:=]\s*([0-9.-]+)',
            r'çµŒåº¦[ï¼š:]\s*([0-9.-]+)'
        ]
        
        for pattern in lat_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['map_lat'] = matches[0]
                break
        
        for pattern in lng_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['map_lng'] = matches[0]
                break
        
        return data
    
    def _extract_from_markdown(self, markdown: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract additional data tá»« markdown content
        """
        if not markdown:
            return data
        
        # Extract address components tá»« markdown
        lines = markdown.split('\n')
        station_count = 1
        
        for line in lines:
            line = line.strip()
            
            # Look for address patterns (more comprehensive)
            prefecture_patterns = [
                'æ±äº¬éƒ½', 'å¤§é˜ªåºœ', 'ç¥å¥ˆå·çœŒ', 'æ„›çŸ¥çœŒ', 'å…µåº«çœŒ', 'ç¦å²¡çœŒ', 
                'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'åŒ—æµ·é“', 'äº¬éƒ½åºœ', 'åºƒå³¶çœŒ', 'å®®åŸçœŒ'
            ]
            
            for prefecture in prefecture_patterns:
                if prefecture in line and not data['prefecture']:
                    data['prefecture'] = prefecture
                    # Extract city and district from the same line
                    remaining = line.split(prefecture)[1] if prefecture in line else line
                    
                    # Extract city (å¸‚)
                    city_match = re.search(r'([^å¸‚]+å¸‚)', remaining)
                    if city_match and not data['city']:
                        data['city'] = city_match.group(1)
                    
                    # Extract district (åŒº)
                    district_match = re.search(r'([^åŒº]+åŒº)', remaining)
                    if district_match and not data['district']:
                        data['district'] = district_match.group(1)
                    
                    # Extract chome-banchi
                    chome_match = re.search(r'(\d+ä¸ç›®\d+ç•ª\d+å·?)', remaining)
                    if chome_match and not data['chome_banchi']:
                        data['chome_banchi'] = chome_match.group(1)
                    break
            
            # Extract multiple station information
            if 'é§…' in line and station_count <= 5:
                # Extract station name
                station_match = re.search(r'([^é§…\s]+é§…)', line)
                if station_match:
                    station_field = f'station_name_{station_count}'
                    if not data[station_field]:
                        data[station_field] = station_match.group(1)
                        
                        # Extract train line
                        line_patterns = [
                            r'([^ç·š\s]+ç·š)',
                            r'JR([^é§…\s]+)',
                            r'(æ±æ€¥[^é§…\s]+)',
                            r'(äº¬æ€¥[^é§…\s]+)',
                            r'(å°ç”°æ€¥[^é§…\s]+)'
                        ]
                        
                        for pattern in line_patterns:
                            line_match = re.search(pattern, line)
                            if line_match:
                                train_line_field = f'train_line_name_{station_count}'
                                if not data[train_line_field]:
                                    data[train_line_field] = line_match.group(1)
                                break
                        
                        # Extract walking time
                        walk_match = re.search(r'å¾’æ­©(\d+)åˆ†', line)
                        if walk_match:
                            walk_field = f'walk_{station_count}'
                            if not data[walk_field]:
                                data[walk_field] = walk_match.group(1)
                        
                        # Extract bus time
                        bus_match = re.search(r'ãƒã‚¹(\d+)åˆ†', line)
                        if bus_match:
                            bus_field = f'bus_{station_count}'
                            if not data[bus_field]:
                                data[bus_field] = bus_match.group(1)
                        
                        # Extract car time
                        car_match = re.search(r'è»Š(\d+)åˆ†', line)
                        if car_match:
                            car_field = f'car_{station_count}'
                            if not data[car_field]:
                                data[car_field] = car_match.group(1)
                        
                        station_count += 1
            
            # Extract building type
            if not data['building_type']:
                building_types = ['ãƒãƒ³ã‚·ãƒ§ãƒ³', 'ã‚¢ãƒ‘ãƒ¼ãƒˆ', 'ä¸€æˆ¸å»ºã¦', 'ãƒ†ãƒ©ã‚¹ãƒã‚¦ã‚¹', 'ã‚¿ã‚¦ãƒ³ãƒã‚¦ã‚¹']
                for building_type in building_types:
                    if building_type in line:
                        data['building_type'] = building_type
                        break
            
            # Extract available date
            if not data['available_from']:
                date_patterns = [
                    r'å…¥å±…å¯èƒ½æ—¥[ï¼š:]\s*([^\n]+)',
                    r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                    r'å³å…¥å±…å¯',
                    r'ç›¸è«‡'
                ]
                
                for pattern in date_patterns:
                    date_match = re.search(pattern, line)
                    if date_match:
                        if pattern == r'å³å…¥å±…å¯':
                            data['available_from'] = 'å³å…¥å±…å¯'
                        elif pattern == r'ç›¸è«‡':
                            data['available_from'] = 'ç›¸è«‡'
                        else:
                            data['available_from'] = date_match.group(1).strip()
                        break
            
            # Extract postcode
            if not data['postcode']:
                postcode_match = re.search(r'ã€’(\d{3}-\d{4})', line)
                if postcode_match:
                    data['postcode'] = postcode_match.group(1)
            
            # Extract unit number
            if not data['unit_no']:
                unit_patterns = [
                    r'(\d+å·å®¤)',
                    r'éƒ¨å±‹ç•ªå·[ï¼š:]\s*(\d+)',
                    r'Unit\s*(\d+)'
                ]
                
                for pattern in unit_patterns:
                    unit_match = re.search(pattern, line, re.IGNORECASE)
                    if unit_match:
                        data['unit_no'] = unit_match.group(1)
                        break
        
        return data
    
    def _generate_property_id(self, url: str) -> str:
        """Táº¡o property ID tá»« URL"""
        # Extract ID tá»« URL
        match = re.search(r'/(\d+)/?$', url)
        if match:
            return f"PROP_{match.group(1)}"
        
        # Extract tá»« path
        path_match = re.search(r'/([^/]+)/?$', url)
        if path_match:
            return f"PROP_{path_match.group(1)}"
        
        # Fallback: timestamp
        return f"PROP_{int(datetime.now().timestamp())}"
    
    def validate_and_create_property_model(self, data: Dict[str, Any]) -> PropertyModel:
        """
        Validate vÃ  táº¡o PropertyModel tá»« extracted data
        """
        try:
            print(f"ğŸ” Creating PropertyModel with data type: {type(data)}")
            print(f"ğŸ” Data keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
            
            # Xá»­ lÃ½ images náº¿u cÃ³
            if 'images' in data and isinstance(data['images'], list):
                print(f"ğŸ” Processing {len(data['images'])} images")
                processed_images = []
                for i, img in enumerate(data['images']):
                    print(f"ğŸ” Image {i}: {type(img)} - {img}")
                    if isinstance(img, dict) and 'url' in img:
                        processed_images.append(PropertyImage(**img))
                    else:
                        print(f"âš ï¸ Skipping invalid image data: {img}")
                data['images'] = processed_images
            
            # Táº¡o PropertyModel
            property_model = PropertyModel(**data)
            return property_model
            
        except Exception as e:
            print(f"âŒ Error creating PropertyModel: {e}")
            print(f"ğŸ” Data causing error: {data}")
            import traceback
            traceback.print_exc()
            
            # Táº¡o model vá»›i dá»¯ liá»‡u cÆ¡ báº£n
            basic_data = {
                'link': data.get('link'),
                'property_csv_id': data.get('property_csv_id'),
                'create_date': data.get('create_date')
            }
            return PropertyModel(**basic_data)


class EnhancedPropertyCrawler:
    """
    Enhanced Property Crawler sá»­ dá»¥ng crawl4ai (khÃ´ng cáº§n LLM)
    """
    
    def __init__(self):
        self.extractor = PropertyExtractor()
    
    async def crawl_property(self, url: str) -> Dict[str, Any]:
        """
        Crawl má»™t property vÃ  tráº£ vá» dá»¯ liá»‡u JSON hoÃ n chá»‰nh
        """
        print(f"ğŸš€ Crawling: {url}")
        
        # Extract dá»¯ liá»‡u báº±ng crawl4ai
        result = await self.extractor.extract_property_data(url)
        
        if result['success']:
            # Validate vÃ  táº¡o PropertyModel
            property_model = self.extractor.validate_and_create_property_model(
                result['property_data']
            )
            
            # Convert vá» dict Ä‘á»ƒ serialize JSON
            result['property_data'] = property_model.dict(exclude_none=True)
            
            print(f"âœ… Success: Extracted {len(result['property_data'])} fields")
        else:
            print(f"âŒ Failed: {result.get('error', 'Unknown error')}")
        
        return result
    
    async def crawl_multiple_properties(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        Crawl nhiá»u properties cÃ¹ng lÃºc
        """
        print(f"ğŸ˜ï¸ Crawling {len(urls)} properties...")
        
        # Táº¡o tasks cho táº¥t cáº£ URLs
        tasks = [self.crawl_property(url) for url in urls]
        
        # Cháº¡y parallel vá»›i error handling
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Xá»­ lÃ½ exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ Error crawling {urls[i]}: {result}")
                processed_results.append({
                    'success': False,
                    'url': urls[i],
                    'error': str(result),
                    'crawl_timestamp': datetime.now().isoformat()
                })
            else:
                processed_results.append(result)
        
        # Thá»‘ng kÃª
        success_count = sum(1 for r in processed_results if r.get('success', False))
        print(f"ğŸ“Š Results: {success_count}/{len(urls)} successful")
        
        return processed_results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], filename: str = None):
        """
        LÆ°u káº¿t quáº£ crawl vÃ o file JSON
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved results to: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ Error saving to JSON: {e}")
            return None
    
    def save_results_to_csv(self, results: List[Dict[str, Any]], filename: str = None):
        """
        LÆ°u káº¿t quáº£ crawl vÃ o file CSV
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.csv"
        
        try:
            import pandas as pd
            
            # Flatten dá»¯ liá»‡u cho CSV
            flattened_data = []
            for result in results:
                if result.get('success') and 'property_data' in result:
                    flat_data = result['property_data'].copy()
                    flat_data['crawl_success'] = True
                    flat_data['crawl_timestamp'] = result.get('crawl_timestamp')
                    flat_data['crawl_url'] = result.get('url')
                    flattened_data.append(flat_data)
                else:
                    # ThÃªm failed records
                    flattened_data.append({
                        'crawl_success': False,
                        'crawl_timestamp': result.get('crawl_timestamp'),
                        'crawl_url': result.get('url'),
                        'crawl_error': result.get('error')
                    })
            
            df = pd.DataFrame(flattened_data)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ğŸ’¾ Saved results to CSV: {filename}")
            return filename
            
        except ImportError:
            print("âŒ pandas not installed. Cannot save to CSV.")
            return None
        except Exception as e:
            print(f"âŒ Error saving to CSV: {e}")
            return None


# Utility functions
async def crawl_single_property(url: str) -> Dict[str, Any]:
    """
    Convenience function Ä‘á»ƒ crawl má»™t property
    """
    crawler = EnhancedPropertyCrawler()
    return await crawler.crawl_property(url)


async def crawl_property_list(urls: List[str]) -> List[Dict[str, Any]]:
    """
    Convenience function Ä‘á»ƒ crawl nhiá»u properties
    """
    crawler = EnhancedPropertyCrawler()
    return await crawler.crawl_multiple_properties(urls)


# Example usage
async def main():
    """
    Example usage cá»§a Enhanced Property Crawler
    """
    time_start = datetime.now().strftime("%Y%m%d_%H%M%S")
    # Test URLs (thay báº±ng URLs thá»±c táº¿)
    test_urls = [
        "https://rent.tokyu-housing-lease.co.jp/rent/8034884/117024"
    ]
    
    # Táº¡o crawler
    crawler = EnhancedPropertyCrawler()
    
    # Crawl single property
    print("=== Testing Single Property Crawl ===")
    if test_urls:
        single_result = await crawler.crawl_property(test_urls[0])
        print(f"Single result: {json.dumps(single_result, indent=2, ensure_ascii=False)}")
    
    # Crawl multiple properties
    print("\n=== Testing Multiple Properties Crawl ===")
    results = await crawler.crawl_multiple_properties(test_urls)
    
    # Save results
    json_file = crawler.save_results_to_json(results)    
    print(f"\n=== Summary ===")
    print(f"Total URLs: {len(test_urls)}")
    print(f"Successful: {sum(1 for r in results if r.get('success'))}")
    print(f"Failed: {sum(1 for r in results if not r.get('success'))}")
    if json_file:
        print(f"JSON saved: {json_file}")
        
    time_end = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    duration = datetime.strptime(time_end, "%Y%m%d_%H%M%S") - datetime.strptime(time_start, "%Y%m%d_%H%M%S")

    print(
        f"Start time: {time_start}, End time: {time_end} ğŸ•’ Duration: {duration}"
    )
    
    # trung bÃ¬nh 16s - 1 ngÃ y 24 tiáº¿ng crawl Ä‘Æ°á»£c 5400 data


if __name__ == "__main__":
    asyncio.run(main())