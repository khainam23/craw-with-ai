"""
Enhanced Property Crawler s·ª≠ d·ª•ng crawl4ai (kh√¥ng c·∫ßn LLM)
T·∫≠n d·ª•ng s·ª©c m·∫°nh c·ªßa crawl4ai ƒë·ªÉ crawl d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from models import PropertyModel, PropertyImage
import time as t


class PropertyExtractor:
    """
    S·ª≠ d·ª•ng crawl4ai ƒë·ªÉ extract d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n (kh√¥ng c·∫ßn LLM)
    """
    
    def __init__(self):
        self.browser_config = BrowserConfig(
            headless=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
    
    async def extract_property_data(self, url: str) -> Dict[str, Any]:
        """
        Extract d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n t·ª´ URL v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin theo PropertyModel
        """
        # C·∫•u h√¨nh crawler ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin
        run_config = CrawlerRunConfig(
            wait_for_images=True,
            delay_before_return_html=3.0,
            page_timeout=45000,
            remove_overlay_elements=True,
            screenshot=True
        )
        
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                result = await crawler.arun(
                    url=url,
                    config=run_config
                )
                
                if result.success:
                    # Extract comprehensive property data
                    extracted_data = self._extract_comprehensive_data(url, result)
                    
                    print(f"‚úÖ Successfully crawled: {url}")
                    print(f"üîç Title: {extracted_data.get('building_name_ja', 'N/A')}")
                    print(f"üîç Extracted {len([k for k, v in extracted_data.items() if v is not None])} fields with data")
                    
                    return {
                        'success': True,
                        'url': url,
                        'property_data': extracted_data,
                        'crawl_timestamp': datetime.now().isoformat(),
                        'raw_html_length': len(result.html) if result.html else 0,
                        'metadata': {
                            'title': result.metadata.get('title', ''),
                            'description': result.metadata.get('description', ''),
                            'keywords': result.metadata.get('keywords', '')
                        }
                    }
                else:
                    return {
                        'success': False,
                        'url': url,
                        'error': result.error_message or 'Failed to extract content',
                        'crawl_timestamp': datetime.now().isoformat()
                    }
                    
        except Exception as e:
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'crawl_timestamp': datetime.now().isoformat()
            }
    
    def _extract_comprehensive_data(self, url: str, result) -> Dict[str, Any]:
        """
        Extract comprehensive property data t·ª´ crawl result
        """
        # Kh·ªüi t·∫°o data structure v·ªõi t·∫•t c·∫£ fields t·ª´ PropertyModel
        extracted_data = {
            # Th√¥ng tin c∆° b·∫£n
            'link': url,
            'property_csv_id': self._generate_property_id(url),
            'create_date': datetime.now().isoformat(),
            
            # ƒê·ªãa ch·ªâ
            'postcode': None,
            'prefecture': None,
            'city': None,
            'district': None,
            'chome_banchi': None,
            
            # Th√¥ng tin t√≤a nh√†
            'building_type': None,
            'year': None,
            
            # T√™n t√≤a nh√† ƒëa ng√¥n ng·ªØ
            'building_name_en': None,
            'building_name_ja': None,
            'building_name_vi': None,
            
            # M√¥ t·∫£ t√≤a nh√† ƒëa ng√¥n ng·ªØ
            'building_description_en': None,
            'building_description_ja': None,
            'building_description_vi': None,
            
            # ƒê·ªãa danh g·∫ßn ƒë√≥ ƒëa ng√¥n ng·ªØ
            'building_landmarks_en': None,
            'building_landmarks_ja': None,
            'building_landmarks_vi': None,
            
            # Th√¥ng tin ga t√†u (5 ga)
            'station_name_1': None, 'train_line_name_1': None, 'walk_1': None, 'bus_1': None, 'car_1': None, 'cycle_1': None,
            'station_name_2': None, 'train_line_name_2': None, 'walk_2': None, 'bus_2': None, 'car_2': None, 'cycle_2': None,
            'station_name_3': None, 'train_line_name_3': None, 'walk_3': None, 'bus_3': None, 'car_3': None, 'cycle_3': None,
            'station_name_4': None, 'train_line_name_4': None, 'walk_4': None, 'bus_4': None, 'car_4': None, 'cycle_4': None,
            'station_name_5': None, 'train_line_name_5': None, 'walk_5': None, 'bus_5': None, 'car_5': None, 'cycle_5': None,
            
            # T·ªça ƒë·ªô ƒë·ªãa l√Ω
            'map_lat': None,
            'map_lng': None,
            
            # Th√¥ng tin c·∫•u tr√∫c t√≤a nh√†
            'num_units': None,
            'floors': None,
            'basement_floors': None,
            
            # Th√¥ng tin ƒë·∫≠u xe
            'parking': None,
            'parking_cost': None,
            'bicycle_parking': None,
            'motorcycle_parking': None,
            
            # Th√¥ng tin c·∫•u tr√∫c v√† phong c√°ch
            'structure': None,
            'building_notes': None,
            'building_style': None,
            
            # Ti·ªán √≠ch t√≤a nh√†
            'autolock': None, 'credit_card': None, 'concierge': None, 'delivery_box': None,
            'elevator': None, 'gym': None, 'newly_built': None, 'pets': None,
            'swimming_pool': None, 'ur': None,
            
            # Th√¥ng tin cƒÉn h·ªô
            'room_type': None,
            'size': None,
            'unit_no': None,
            'ad_type': None,
            'available_from': None,
            
            # M√¥ t·∫£ b·∫•t ƒë·ªông s·∫£n ƒëa ng√¥n ng·ªØ
            'property_description_en': None,
            'property_description_ja': None,
            'property_description_vi': None,
            
            # Chi ph√≠ kh√°c ƒëa ng√¥n ng·ªØ
            'property_other_expenses_en': None,
            'property_other_expenses_ja': None,
            'property_other_expenses_vi': None,
            
            # Lo·∫°i n·ªïi b·∫≠t
            'featured_a': None, 'featured_b': None, 'featured_c': None,
            
            # Th√¥ng tin t·∫ßng v√† gi√° thu√™
            'floor_no': None,
            'monthly_rent': None,
            'monthly_maintenance': None,
            
            # C√°c kho·∫£n ph√≠
            'months_deposit': None, 'numeric_deposit': None,
            'months_key': None, 'numeric_key': None,
            'months_guarantor': None, 'numeric_guarantor': None,
            'months_agency': None, 'numeric_agency': None,
            'months_renewal': None, 'numeric_renewal': None,
            'months_deposit_amortization': None, 'numeric_deposit_amortization': None,
            'months_security_deposit': None, 'numeric_security_deposit': None,
            
            # C√°c ph√≠ kh√°c
            'lock_exchange': None,
            'fire_insurance': None,
            'other_initial_fees': None,
            'other_subscription_fees': None,
            
            # Th√¥ng tin b·∫£o l√£nh
            'no_guarantor': None,
            'guarantor_agency': None,
            'guarantor_agency_name': None,
            'numeric_guarantor_max': None,
            
            # Th√¥ng tin thu√™
            'rent_negotiable': None,
            'renewal_new_rent': None,
            'lease_date': None,
            'lease_months': None,
            'lease_type': None,
            'short_term_ok': None,
            
            # Th√¥ng tin ban c√¥ng v√† ghi ch√∫
            'balcony_size': None,
            'property_notes': None,
            'discount': None,
            
            # H∆∞·ªõng cƒÉn h·ªô
            'facing_north': None, 'facing_northeast': None, 'facing_east': None, 'facing_southeast': None,
            'facing_south': None, 'facing_southwest': None, 'facing_west': None, 'facing_northwest': None,
            
            # Ti·ªán nghi cƒÉn h·ªô (r·∫•t nhi·ªÅu)
            'aircon': None, 'aircon_heater': None, 'all_electric': None, 'auto_fill_bath': None,
            'balcony': None, 'bath': None, 'bath_water_heater': None, 'blinds': None,
            'bs': None, 'cable': None, 'carpet': None, 'cleaning_service': None,
            'counter_kitchen': None, 'dishwasher': None, 'drapes': None, 'female_only': None,
            'fireplace': None, 'flooring': None, 'full_kitchen': None, 'furnished': None,
            'gas': None, 'induction_cooker': None, 'internet_broadband': None, 'internet_wifi': None,
            'japanese_toilet': None, 'linen': None, 'loft': None, 'microwave': None,
            'oven': None, 'phoneline': None, 'range': None, 'refrigerator': None,
            'refrigerator_freezer': None, 'roof_balcony': None, 'separate_toilet': None, 'shower': None,
            'soho': None, 'storage': None, 'student_friendly': None, 'system_kitchen': None,
            'tatami': None, 'underfloor_heating': None, 'unit_bath': None, 'utensils_cutlery': None,
            'veranda': None, 'washer_dryer': None, 'washing_machine': None, 'washlet': None,
            'western_toilet': None, 'yard': None,
            
            # Media links
            'youtube': None,
            'vr_link': None,
            
            # H√¨nh ·∫£nh
            'images': []
        }
        
        # Extract data t·ª´ HTML content
        html_content = result.html if result.html else ""
        markdown_content = result.markdown if result.markdown else ""
        
        # Extract basic info t·ª´ metadata
        metadata = result.metadata or {}
        title = metadata.get('title', '')
        description = metadata.get('description', '')
        
        # Set building name t·ª´ title
        if title:
            extracted_data['building_name_ja'] = title
            extracted_data['building_name_en'] = title  # C√≥ th·ªÉ translate sau
        
        # Set description
        if description:
            extracted_data['property_description_ja'] = description
            extracted_data['property_description_en'] = description  # C√≥ th·ªÉ translate sau
        
        # Extract images t·ª´ HTML content
        images = self._extract_images_from_html(html_content)
        extracted_data['images'] = images
        
        # Extract structured data t·ª´ HTML patterns
        extracted_data = self._extract_from_html_patterns(html_content, extracted_data)
        
        # Extract t·ª´ markdown content
        extracted_data = self._extract_from_markdown(markdown_content, extracted_data)
        
        return extracted_data
    
    def _extract_images_from_html(self, html: str) -> List[Dict[str, str]]:
        """
        Extract images t·ª´ HTML content
        """
        if not html:
            return []
        
        images = []
        
        # Extract img tags with src attributes
        img_patterns = [
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*alt=["\']([^"\']*)["\'][^>]*>',
            r'<img[^>]+alt=["\']([^"\']*)["\'][^>]*src=["\']([^"\']+)["\'][^>]*>',
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        ]
        
        for pattern in img_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) == 2:
                        # Pattern with both src and alt
                        if 'src=' in pattern and pattern.index('src=') < pattern.index('alt='):
                            src, alt = match
                        else:
                            alt, src = match
                    else:
                        src = match[0]
                        alt = ""
                else:
                    src = match
                    alt = ""
                
                # Filter out small icons and invalid URLs
                if (src and 
                    not any(skip in src.lower() for skip in ['icon', 'logo', 'button', 'arrow']) and
                    (src.startswith('http') or src.startswith('/')) and
                    any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif'])):
                    
                    # Make relative URLs absolute
                    if src.startswith('/'):
                        # Extract domain from the current URL if needed
                        src = src  # Keep as is for now, can be enhanced later
                    
                    image_data = {
                        'url': src,
                        'category': self._categorize_image(alt, len(images))
                    }
                    images.append(image_data)
                    
                    # Limit to 20 images
                    if len(images) >= 20:
                        break
            
            if len(images) >= 20:
                break
        
        return images
    
    def _categorize_image(self, alt_text: str, index: int) -> str:
        """
        Categorize image d·ª±a tr√™n alt text ho·∫∑c index
        """
        alt_lower = alt_text.lower() if alt_text else ""
        
        # Japanese keywords for categorization
        if any(word in alt_lower for word in ['exterior', 'outside', 'building', 'facade', 'Â§ñË¶≥', 'Âª∫Áâ©']):
            return 'exterior'
        elif any(word in alt_lower for word in ['interior', 'room', 'living', 'bedroom', 'ÂÆ§ÂÜÖ', 'ÈÉ®Â±ã', '„É™„Éì„É≥„Ç∞']):
            return 'interior'
        elif any(word in alt_lower for word in ['kitchen', 'dining', '„Ç≠„ÉÉ„ÉÅ„É≥', 'Âè∞ÊâÄ']):
            return 'kitchen'
        elif any(word in alt_lower for word in ['bathroom', 'bath', 'toilet', '„Éê„Çπ', '„Éà„Ç§„É¨', 'Êµ¥ÂÆ§']):
            return 'bathroom'
        elif any(word in alt_lower for word in ['balcony', 'terrace', 'veranda', '„Éê„É´„Ç≥„Éã„Éº', '„Éô„É©„É≥„ÉÄ']):
            return 'balcony'
        else:
            # Default categorization based on index
            categories = ['exterior', 'interior', 'kitchen', 'bathroom', 'balcony', 'other']
            return categories[index % len(categories)]
    
    def _extract_from_html_patterns(self, html: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract data t·ª´ HTML patterns (regex-based)
        """
        if not html:
            return data
        
        # Extract rent price patterns
        rent_patterns = [
            r'¬•([\d,]+)',
            r'(\d+)‰∏áÂÜÜ',
            r'(\d+),(\d+)ÂÜÜ'
        ]
        
        for pattern in rent_patterns:
            matches = re.findall(pattern, html)
            if matches:
                try:
                    if isinstance(matches[0], tuple):
                        # Handle comma-separated numbers
                        rent_value = ''.join(matches[0])
                    else:
                        rent_value = matches[0].replace(',', '')
                    data['monthly_rent'] = rent_value
                    break
                except:
                    continue
        
        # Extract room type patterns
        room_patterns = [
            r'(\d+[LDKS]+)',
            r'(\d+K)',
            r'(\d+DK)',
            r'(\d+LDK)'
        ]
        
        for pattern in room_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['room_type'] = matches[0]
                break
        
        # Extract size patterns
        size_patterns = [
            r'(\d+\.?\d*)„é°',
            r'(\d+\.?\d*)m¬≤',
            r'(\d+\.?\d*)\s*Âπ≥Á±≥'
        ]
        
        for pattern in size_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['size'] = matches[0]
                break
        
        # Extract floor patterns
        floor_patterns = [
            r'(\d+)Èöé',
            r'(\d+)F'
        ]
        
        for pattern in floor_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['floor_no'] = matches[0]
                break
        
        # Extract year patterns
        year_patterns = [
            r'ÁØâ(\d{4})Âπ¥',
            r'(\d{4})Âπ¥ÁØâ',
            r'Âª∫ÁØâÂπ¥.*?(\d{4})'
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, html)
            if matches:
                data['year'] = matches[0]
                break
        
        # Extract boolean amenities (Y/N fields) - comprehensive list
        amenity_keywords = {
            # Building amenities
            'elevator': ['„Ç®„É¨„Éô„Éº„Çø„Éº', 'elevator', 'EV'],
            'autolock': ['„Ç™„Éº„Éà„É≠„ÉÉ„ÇØ', 'auto lock', 'autoloc'],
            'delivery_box': ['ÂÆÖÈÖç„Éú„ÉÉ„ÇØ„Çπ', 'delivery box', 'ÂÆÖÈÖçBOX'],
            'concierge': ['„Ç≥„É≥„Ç∑„Çß„É´„Ç∏„É•', 'concierge', '„Éï„É≠„É≥„Éà'],
            'gym': ['„Ç∏„É†', 'gym', '„Éï„Ç£„ÉÉ„Éà„Éç„Çπ'],
            'swimming_pool': ['„Éó„Éº„É´', 'pool', 'swimming'],
            
            # Parking
            'parking': ['ÈßêËªäÂ†¥', 'parking', 'ÈßêËªä'],
            'bicycle_parking': ['ÈßêËº™Â†¥', 'bicycle parking', 'Ëá™Ëª¢Ëªä'],
            'motorcycle_parking': ['„Éê„Ç§„ÇØÁΩÆÂ†¥', 'motorcycle', '„Éê„Ç§„ÇØ'],
            
            # Unit amenities
            'aircon': ['„Ç®„Ç¢„Ç≥„É≥', 'air conditioning', 'aircon', 'AC'],
            'aircon_heater': ['„Ç®„Ç¢„Ç≥„É≥ÊöñÊàø', 'air conditioning heater'],
            'internet_wifi': ['WiFi', '„Ç§„É≥„Çø„Éº„Éç„ÉÉ„Éà', 'internet', '„Éç„ÉÉ„Éà'],
            'cable': ['„Ç±„Éº„Éñ„É´TV', 'cable', 'CATV'],
            'bs': ['BS', 'BSÊîæÈÄÅ', 'satellite'],
            
            # Kitchen
            'system_kitchen': ['„Ç∑„Çπ„ÉÜ„É†„Ç≠„ÉÉ„ÉÅ„É≥', 'system kitchen'],
            'counter_kitchen': ['„Ç´„Ç¶„É≥„Çø„Éº„Ç≠„ÉÉ„ÉÅ„É≥', 'counter kitchen'],
            'full_kitchen': ['„Éï„É´„Ç≠„ÉÉ„ÉÅ„É≥', 'full kitchen'],
            'induction_cooker': ['IH„ÇØ„ÉÉ„Ç≠„É≥„Ç∞', 'induction', 'IH'],
            'gas': ['„Ç¨„Çπ', 'gas'],
            'microwave': ['ÈõªÂ≠ê„É¨„É≥„Ç∏', 'microwave'],
            'oven': ['„Ç™„Éº„Éñ„É≥', 'oven'],
            'dishwasher': ['È£üÊ¥óÊ©ü', 'dishwasher', 'È£üÂô®Ê¥ó„ÅÑ'],
            'refrigerator': ['ÂÜ∑ËîµÂ∫´', 'refrigerator', 'ÂÜ∑Ëîµ'],
            'refrigerator_freezer': ['ÂÜ∑ÂáçÂÜ∑ËîµÂ∫´', 'freezer'],
            
            # Bathroom
            'bath': ['„Éê„Çπ', 'bath', 'Êµ¥ÂÆ§'],
            'separate_toilet': ['Áã¨Á´ãÊ¥óÈù¢Âè∞', 'separate toilet', 'Áã¨Á´ã'],
            'unit_bath': ['„É¶„Éã„ÉÉ„Éà„Éê„Çπ', 'unit bath'],
            'auto_fill_bath': ['Ëá™ÂãïÁµ¶ÊπØ', 'auto fill'],
            'shower': ['„Ç∑„É£„ÉØ„Éº', 'shower'],
            'japanese_toilet': ['ÂíåÂºè„Éà„Ç§„É¨', 'japanese toilet'],
            'western_toilet': ['Ê¥ãÂºè„Éà„Ç§„É¨', 'western toilet'],
            'washlet': ['„Ç¶„Ç©„Ç∑„É•„É¨„ÉÉ„Éà', 'washlet'],
            
            # Flooring & Interior
            'flooring': ['„Éï„É≠„Éº„É™„É≥„Ç∞', 'flooring', '„Éï„É≠„Ç¢'],
            'tatami': ['Áï≥', 'tatami'],
            'carpet': ['„Ç´„Éº„Éö„ÉÉ„Éà', 'carpet'],
            'underfloor_heating': ['Â∫äÊöñÊàø', 'underfloor heating'],
            
            # Storage & Space
            'storage': ['ÂèéÁ¥ç', 'storage', '„ÇØ„É≠„Éº„Çº„ÉÉ„Éà'],
            'loft': ['„É≠„Éï„Éà', 'loft'],
            'balcony': ['„Éê„É´„Ç≥„Éã„Éº', 'balcony'],
            'veranda': ['„Éô„É©„É≥„ÉÄ', 'veranda'],
            'roof_balcony': ['„É´„Éº„Éï„Éê„É´„Ç≥„Éã„Éº', 'roof balcony'],
            'yard': ['Â∫≠', 'yard', '„Ç¨„Éº„Éá„É≥'],
            
            # Appliances
            'washing_machine': ['Ê¥óÊøØÊ©ü', 'washing machine'],
            'washer_dryer': ['Ê¥óÊøØ‰πæÁá•Ê©ü', 'washer dryer'],
            'furnished': ['ÂÆ∂ÂÖ∑‰ªò„Åç', 'furnished', 'ÂÆ∂ÂÖ∑'],
            'all_electric': ['„Ç™„Éº„É´ÈõªÂåñ', 'all electric'],
            
            # Special features
            'pets': ['„Éö„ÉÉ„Éà', 'pet', '„Éö„ÉÉ„ÉàÂèØ'],
            'female_only': ['Â•≥ÊÄßÈôêÂÆö', 'female only', 'Â•≥ÊÄßÂ∞ÇÁî®'],
            'student_friendly': ['Â≠¶ÁîüÂèØ', 'student', 'Â≠¶Áîü'],
            'soho': ['SOHO', 'soho', '‰∫ãÂãôÊâÄÂèØ'],
            'newly_built': ['Êñ∞ÁØâ', 'newly built', 'Êñ∞ÁØâÁâ©‰ª∂']
        }
        
        for field, keywords in amenity_keywords.items():
            for keyword in keywords:
                if keyword in html:
                    data[field] = 'Y'
                    break
            if data[field] != 'Y':
                data[field] = None  # Keep as None if not found
        
        # Extract additional pricing information
        deposit_patterns = [
            r'Êï∑Èáë[Ôºö:]\s*(\d+)‰∏áÂÜÜ',
            r'Êï∑Èáë[Ôºö:]\s*(\d+)„É∂?Êúà',
            r'deposit[Ôºö:]\s*¬•([\d,]+)'
        ]
        
        for pattern in deposit_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['numeric_deposit'] = matches[0].replace(',', '')
                break
        
        # Extract key money patterns
        key_patterns = [
            r'Á§ºÈáë[Ôºö:]\s*(\d+)‰∏áÂÜÜ',
            r'Á§ºÈáë[Ôºö:]\s*(\d+)„É∂?Êúà',
            r'key money[Ôºö:]\s*¬•([\d,]+)'
        ]
        
        for pattern in key_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['numeric_key'] = matches[0].replace(',', '')
                break
        
        # Extract maintenance fee patterns
        maintenance_patterns = [
            r'ÁÆ°ÁêÜË≤ª[Ôºö:]\s*(\d+)ÂÜÜ',
            r'ÂÖ±ÁõäË≤ª[Ôºö:]\s*(\d+)ÂÜÜ',
            r'maintenance[Ôºö:]\s*¬•([\d,]+)'
        ]
        
        for pattern in maintenance_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['monthly_maintenance'] = matches[0].replace(',', '')
                break
        
        # Extract building structure
        structure_patterns = [
            r'ÊßãÈÄ†[Ôºö:]\s*([^<\n]+)',
            r'(RC|SRC|Êú®ÈÄ†|ÈâÑÈ™®|ËªΩÈáèÈâÑÈ™®)',
            r'structure[Ôºö:]\s*([^<\n]+)'
        ]
        
        for pattern in structure_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['structure'] = matches[0].strip()
                break
        
        # Extract building floors
        building_floor_patterns = [
            r'Âú∞‰∏ä(\d+)Èöé',
            r'(\d+)ÈöéÂª∫',
            r'building.*?(\d+)\s*floors?'
        ]
        
        for pattern in building_floor_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['floors'] = matches[0]
                break
        
        # Extract coordinates from maps or scripts
        lat_patterns = [
            r'lat["\']?\s*[:=]\s*([0-9.-]+)',
            r'latitude["\']?\s*[:=]\s*([0-9.-]+)',
            r'Á∑ØÂ∫¶[Ôºö:]\s*([0-9.-]+)'
        ]
        
        lng_patterns = [
            r'lng["\']?\s*[:=]\s*([0-9.-]+)',
            r'longitude["\']?\s*[:=]\s*([0-9.-]+)',
            r'ÁµåÂ∫¶[Ôºö:]\s*([0-9.-]+)'
        ]
        
        for pattern in lat_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['map_lat'] = matches[0]
                break
        
        for pattern in lng_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                data['map_lng'] = matches[0]
                break
        
        return data
    
    def _extract_from_markdown(self, markdown: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract additional data t·ª´ markdown content
        """
        if not markdown:
            return data
        
        # Extract address components t·ª´ markdown
        lines = markdown.split('\n')
        station_count = 1
        
        for line in lines:
            line = line.strip()
            
            # Look for address patterns (more comprehensive)
            prefecture_patterns = [
                'Êù±‰∫¨ÈÉΩ', 'Â§ßÈò™Â∫ú', 'Á•ûÂ•àÂ∑ùÁúå', 'ÊÑõÁü•Áúå', 'ÂÖµÂ∫´Áúå', 'Á¶èÂ≤°Áúå', 
                'ÂüºÁéâÁúå', 'ÂçÉËëâÁúå', 'ÂåóÊµ∑ÈÅì', '‰∫¨ÈÉΩÂ∫ú', 'Â∫ÉÂ≥∂Áúå', 'ÂÆÆÂüéÁúå'
            ]
            
            for prefecture in prefecture_patterns:
                if prefecture in line and not data['prefecture']:
                    data['prefecture'] = prefecture
                    # Extract city and district from the same line
                    remaining = line.split(prefecture)[1] if prefecture in line else line
                    
                    # Extract city (Â∏Ç)
                    city_match = re.search(r'([^Â∏Ç]+Â∏Ç)', remaining)
                    if city_match and not data['city']:
                        data['city'] = city_match.group(1)
                    
                    # Extract district (Âå∫)
                    district_match = re.search(r'([^Âå∫]+Âå∫)', remaining)
                    if district_match and not data['district']:
                        data['district'] = district_match.group(1)
                    
                    # Extract chome-banchi
                    chome_match = re.search(r'(\d+‰∏ÅÁõÆ\d+Áï™\d+Âè∑?)', remaining)
                    if chome_match and not data['chome_banchi']:
                        data['chome_banchi'] = chome_match.group(1)
                    break
            
            # Extract multiple station information
            if 'ÈßÖ' in line and station_count <= 5:
                # Extract station name
                station_match = re.search(r'([^ÈßÖ\s]+ÈßÖ)', line)
                if station_match:
                    station_field = f'station_name_{station_count}'
                    if not data[station_field]:
                        data[station_field] = station_match.group(1)
                        
                        # Extract train line
                        line_patterns = [
                            r'([^Á∑ö\s]+Á∑ö)',
                            r'JR([^ÈßÖ\s]+)',
                            r'(Êù±ÊÄ•[^ÈßÖ\s]+)',
                            r'(‰∫¨ÊÄ•[^ÈßÖ\s]+)',
                            r'(Â∞èÁî∞ÊÄ•[^ÈßÖ\s]+)'
                        ]
                        
                        for pattern in line_patterns:
                            line_match = re.search(pattern, line)
                            if line_match:
                                train_line_field = f'train_line_name_{station_count}'
                                if not data[train_line_field]:
                                    data[train_line_field] = line_match.group(1)
                                break
                        
                        # Extract walking time
                        walk_match = re.search(r'ÂæíÊ≠©(\d+)ÂàÜ', line)
                        if walk_match:
                            walk_field = f'walk_{station_count}'
                            if not data[walk_field]:
                                data[walk_field] = walk_match.group(1)
                        
                        # Extract bus time
                        bus_match = re.search(r'„Éê„Çπ(\d+)ÂàÜ', line)
                        if bus_match:
                            bus_field = f'bus_{station_count}'
                            if not data[bus_field]:
                                data[bus_field] = bus_match.group(1)
                        
                        # Extract car time
                        car_match = re.search(r'Ëªä(\d+)ÂàÜ', line)
                        if car_match:
                            car_field = f'car_{station_count}'
                            if not data[car_field]:
                                data[car_field] = car_match.group(1)
                        
                        station_count += 1
            
            # Extract building type
            if not data['building_type']:
                building_types = ['„Éû„É≥„Ç∑„Éß„É≥', '„Ç¢„Éë„Éº„Éà', '‰∏ÄÊà∏Âª∫„Å¶', '„ÉÜ„É©„Çπ„Éè„Ç¶„Çπ', '„Çø„Ç¶„É≥„Éè„Ç¶„Çπ']
                for building_type in building_types:
                    if building_type in line:
                        data['building_type'] = building_type
                        break
            
            # Extract available date
            if not data['available_from']:
                date_patterns = [
                    r'ÂÖ•Â±ÖÂèØËÉΩÊó•[Ôºö:]\s*([^\n]+)',
                    r'(\d{4}Âπ¥\d{1,2}Êúà\d{1,2}Êó•)',
                    r'Âç≥ÂÖ•Â±ÖÂèØ',
                    r'Áõ∏Ë´á'
                ]
                
                for pattern in date_patterns:
                    date_match = re.search(pattern, line)
                    if date_match:
                        if pattern == r'Âç≥ÂÖ•Â±ÖÂèØ':
                            data['available_from'] = 'Âç≥ÂÖ•Â±ÖÂèØ'
                        elif pattern == r'Áõ∏Ë´á':
                            data['available_from'] = 'Áõ∏Ë´á'
                        else:
                            data['available_from'] = date_match.group(1).strip()
                        break
            
            # Extract postcode
            if not data['postcode']:
                postcode_match = re.search(r'„Äí(\d{3}-\d{4})', line)
                if postcode_match:
                    data['postcode'] = postcode_match.group(1)
            
            # Extract unit number
            if not data['unit_no']:
                unit_patterns = [
                    r'(\d+Âè∑ÂÆ§)',
                    r'ÈÉ®Â±ãÁï™Âè∑[Ôºö:]\s*(\d+)',
                    r'Unit\s*(\d+)'
                ]
                
                for pattern in unit_patterns:
                    unit_match = re.search(pattern, line, re.IGNORECASE)
                    if unit_match:
                        data['unit_no'] = unit_match.group(1)
                        break
        
        return data
    
    def _generate_property_id(self, url: str) -> str:
        """T·∫°o property ID t·ª´ URL"""
        # Extract ID t·ª´ URL
        match = re.search(r'/(\d+)/?$', url)
        if match:
            return f"PROP_{match.group(1)}"
        
        # Extract t·ª´ path
        path_match = re.search(r'/([^/]+)/?$', url)
        if path_match:
            return f"PROP_{path_match.group(1)}"
        
        # Fallback: timestamp
        return f"PROP_{int(datetime.now().timestamp())}"
    
    def validate_and_create_property_model(self, data: Dict[str, Any]) -> PropertyModel:
        """
        Validate v√† t·∫°o PropertyModel t·ª´ extracted data
        """
        try:
            print(f"üîç Creating PropertyModel with data type: {type(data)}")
            print(f"üîç Data keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
            
            # X·ª≠ l√Ω images n·∫øu c√≥
            if 'images' in data and isinstance(data['images'], list):
                print(f"üîç Processing {len(data['images'])} images")
                processed_images = []
                for i, img in enumerate(data['images']):
                    print(f"üîç Image {i}: {type(img)} - {img}")
                    if isinstance(img, dict) and 'url' in img:
                        processed_images.append(PropertyImage(**img))
                    else:
                        print(f"‚ö†Ô∏è Skipping invalid image data: {img}")
                data['images'] = processed_images
            
            # T·∫°o PropertyModel
            property_model = PropertyModel(**data)
            return property_model
            
        except Exception as e:
            print(f"‚ùå Error creating PropertyModel: {e}")
            print(f"üîç Data causing error: {data}")
            import traceback
            traceback.print_exc()
            
            # T·∫°o model v·ªõi d·ªØ li·ªáu c∆° b·∫£n
            basic_data = {
                'link': data.get('link'),
                'property_csv_id': data.get('property_csv_id'),
                'create_date': data.get('create_date')
            }
            return PropertyModel(**basic_data)


class EnhancedPropertyCrawler:
    """
    Enhanced Property Crawler s·ª≠ d·ª•ng crawl4ai (kh√¥ng c·∫ßn LLM)
    """
    
    def __init__(self):
        self.extractor = PropertyExtractor()
    
    async def crawl_property(self, url: str) -> Dict[str, Any]:
        """
        Crawl m·ªôt property v√† tr·∫£ v·ªÅ d·ªØ li·ªáu JSON ho√†n ch·ªânh
        """
        print(f"üöÄ Crawling: {url}")
        
        # Extract d·ªØ li·ªáu b·∫±ng crawl4ai
        result = await self.extractor.extract_property_data(url)
        
        if result['success']:
            # Validate v√† t·∫°o PropertyModel
            property_model = self.extractor.validate_and_create_property_model(
                result['property_data']
            )
            
            # Convert v·ªÅ dict ƒë·ªÉ serialize JSON
            result['property_data'] = property_model.dict(exclude_none=True)
            
            print(f"‚úÖ Success: Extracted {len(result['property_data'])} fields")
        else:
            print(f"‚ùå Failed: {result.get('error', 'Unknown error')}")
        
        return result
    
    async def crawl_multiple_properties(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        Crawl nhi·ªÅu properties c√πng l√∫c
        """
        print(f"üèòÔ∏è Crawling {len(urls)} properties...")
        
        # T·∫°o tasks cho t·∫•t c·∫£ URLs
        tasks = [self.crawl_property(url) for url in urls]
        
        # Ch·∫°y parallel v·ªõi error handling
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # X·ª≠ l√Ω exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"‚ùå Error crawling {urls[i]}: {result}")
                processed_results.append({
                    'success': False,
                    'url': urls[i],
                    'error': str(result),
                    'crawl_timestamp': datetime.now().isoformat()
                })
            else:
                processed_results.append(result)
        
        # Th·ªëng k√™
        success_count = sum(1 for r in processed_results if r.get('success', False))
        print(f"üìä Results: {success_count}/{len(urls)} successful")
        
        return processed_results
    
    def save_results_to_json(self, results: List[Dict[str, Any]], filename: str = None):
        """
        L∆∞u k·∫øt qu·∫£ crawl v√†o file JSON
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"üíæ Saved results to: {filename}")
            return filename
        except Exception as e:
            print(f"‚ùå Error saving to JSON: {e}")
            return None
    
    def save_results_to_csv(self, results: List[Dict[str, Any]], filename: str = None):
        """
        L∆∞u k·∫øt qu·∫£ crawl v√†o file CSV
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.csv"
        
        try:
            import pandas as pd
            
            # Flatten d·ªØ li·ªáu cho CSV
            flattened_data = []
            for result in results:
                if result.get('success') and 'property_data' in result:
                    flat_data = result['property_data'].copy()
                    flat_data['crawl_success'] = True
                    flat_data['crawl_timestamp'] = result.get('crawl_timestamp')
                    flat_data['crawl_url'] = result.get('url')
                    flattened_data.append(flat_data)
                else:
                    # Th√™m failed records
                    flattened_data.append({
                        'crawl_success': False,
                        'crawl_timestamp': result.get('crawl_timestamp'),
                        'crawl_url': result.get('url'),
                        'crawl_error': result.get('error')
                    })
            
            df = pd.DataFrame(flattened_data)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"üíæ Saved results to CSV: {filename}")
            return filename
            
        except ImportError:
            print("‚ùå pandas not installed. Cannot save to CSV.")
            return None
        except Exception as e:
            print(f"‚ùå Error saving to CSV: {e}")
            return None


# Utility functions
async def crawl_single_property(url: str) -> Dict[str, Any]:
    """
    Convenience function ƒë·ªÉ crawl m·ªôt property
    """
    crawler = EnhancedPropertyCrawler()
    return await crawler.crawl_property(url)


async def crawl_property_list(urls: List[str]) -> List[Dict[str, Any]]:
    """
    Convenience function ƒë·ªÉ crawl nhi·ªÅu properties
    """
    crawler = EnhancedPropertyCrawler()
    return await crawler.crawl_multiple_properties(urls)


# Example usage
async def main():
    """
    Example usage c·ªßa Enhanced Property Crawler
    """
    time_start = datetime.now().strftime("%Y%m%d_%H%M%S")
    # Test URLs (thay b·∫±ng URLs th·ª±c t·∫ø)
    test_urls = [
        "https://rent.tokyu-housing-lease.co.jp/rent/8034884/117024"
    ]
    
    # T·∫°o crawler
    crawler = EnhancedPropertyCrawler()
    
    # Crawl single property
    print("=== Testing Single Property Crawl ===")
    if test_urls:
        single_result = await crawler.crawl_property(test_urls[0])
        print(f"Single result: {json.dumps(single_result, indent=2, ensure_ascii=False)}")
    
    # Crawl multiple properties
    print("\n=== Testing Multiple Properties Crawl ===")
    results = await crawler.crawl_multiple_properties(test_urls)
    
    # Save results
    json_file = crawler.save_results_to_json(results)    
    print(f"\n=== Summary ===")
    print(f"Total URLs: {len(test_urls)}")
    print(f"Successful: {sum(1 for r in results if r.get('success'))}")
    print(f"Failed: {sum(1 for r in results if not r.get('success'))}")
    if json_file:
        print(f"JSON saved: {json_file}")
        
    time_end = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    duration = datetime.strptime(time_end, "%Y%m%d_%H%M%S") - datetime.strptime(time_start, "%Y%m%d_%H%M%S")

    print(
        f"Start time: {time_start}, End time: {time_end} üïí Duration: {duration}"
    )
    
    # trung b√¨nh 16s - 1 ng√†y 24 ti·∫øng crawl ƒë∆∞·ª£c 5400 data


if __name__ == "__main__":
    asyncio.run(main())