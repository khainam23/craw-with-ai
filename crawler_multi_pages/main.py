import asyncio
from datetime import datetime
from .integrated_crawler import IntegratedPropertyCrawler
from .crawler_config import get_config_by_name, print_config_info

async def main():
    """Main function to run integrated property crawler with configurable settings"""
    
    print("ğŸš€ Starting Integrated Property Crawler - Optimized Version")
    print("="*60)
    
    # Show available configurations
    print_config_info()
    print()
    
    # Choose configuration based on your needs
    # Options: 'conservative', 'balanced', 'aggressive', 'testing', 'unlimited'
    config_name = 'unlimited'  # Change this to suit your needs
    config = get_config_by_name(config_name)
    
    print(f"ğŸ”§ Using '{config_name.upper()}' configuration")
    print("="*40)
    
    # Main crawling configuration
    listing_url = "https://www.mitsui-chintai.co.jp/rf/result?are=36&get=area"
    url_pattern = r"/rf/tatemono/\d+"
    
    print(f"ğŸ“‹ Crawling Configuration:")
    print(f"   ğŸ”— Listing URL: {listing_url}")
    print(f"   ğŸ¯ URL Pattern: {url_pattern}")
    print(f"   ğŸ“„ Max Pages: {config.max_pages}")
    print(f"   ğŸ  Max Properties: {config.max_properties or 'Unlimited'}")
    print(f"   â±ï¸ Delay: {config.delay}s")
    print(f"   ğŸ”„ Max Concurrent: {config.max_concurrent}")
    print(f"   ğŸ“¦ Batch Size: {config.batch_size}")
    print(f"   ğŸ’¾ Memory Threshold: {config.memory_threshold}%")
    print(f"   ğŸ’¾ Save Intermediate: {config.save_intermediate}")
    print()
    
    # Create crawler with configuration
    integrated_crawler = IntegratedPropertyCrawler(
        max_concurrent=config.max_concurrent,
        batch_size=config.batch_size,
        memory_threshold=config.memory_threshold
    )
    
    try:
        # Run the integrated crawling process
        extracted_urls, results, json_file = await integrated_crawler.crawl_from_listing(
            listing_url=listing_url,
            url_pattern=url_pattern,
            max_pages=config.max_pages,
            max_properties=config.max_properties,
            delay=config.delay,
            save_intermediate=config.save_intermediate
        )
        
        # Display sample results
        if results and any(r.get("success") for r in results):
            print(f"\nğŸ” Sample Crawled Properties:")
            print("-" * 40)
            
            successful_results = [r for r in results if r.get("success") and r.get("data")]
            
            for i, result in enumerate(successful_results[:3], 1):  # Show first 3 successful
                data = result["data"]
                print(f"\n  ğŸ  Property {i}:")
                print(f"     URL: {result.get('url', 'N/A')}")
                print(f"     Title: {data.get('title', 'N/A')[:50]}...")
                print(f"     Rent: {data.get('rent', 'N/A')}")
                print(f"     Location: {data.get('location', 'N/A')}")
                print(f"     Size: {data.get('size', 'N/A')}")
                
            if len(successful_results) > 3:
                print(f"\n     ... and {len(successful_results) - 3} more successful properties")
        
        print(f"\nâœ… Crawling completed successfully!")
        if json_file:
            print(f"ğŸ’¾ Results saved to: {json_file}")
        
        # Performance summary
        total_properties = len(results)
        successful_properties = sum(1 for r in results if r.get("success"))
        success_rate = (successful_properties / total_properties * 100) if total_properties > 0 else 0
        
        print(f"\nğŸ“Š Performance Summary:")
        print(f"   ğŸ¯ Success Rate: {success_rate:.1f}% ({successful_properties}/{total_properties})")
        print(f"   âš™ï¸ Configuration Used: {config_name.upper()}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Crawling interrupted by user")
        print("ğŸ’¡ Tip: Intermediate results may have been saved if enabled")
    except Exception as e:
        print(f"\nâŒ Error during crawling: {e}")
        print(f"   Please check your internet connection and URL configuration")
        print(f"   Consider using 'conservative' configuration for unstable connections")

def show_usage_tips():
    """Show usage tips for different scenarios"""
    print("\nğŸ’¡ Usage Tips:")
    print("="*30)
    print("ğŸ”§ Configuration Selection:")
    print("   â€¢ 'testing' - For small tests (20 properties max)")
    print("   â€¢ 'conservative' - For slow/unstable connections")
    print("   â€¢ 'balanced' - For normal usage (recommended)")
    print("   â€¢ 'aggressive' - For high-performance systems")
    print("   â€¢ 'unlimited' - For crawling without page/property limits")
    print()
    print("âš ï¸ Memory Management:")
    print("   â€¢ Monitor system resources during large crawls")
    print("   â€¢ Intermediate saves prevent data loss")
    print("   â€¢ Lower batch_size if experiencing memory issues")
    print()
    print("ğŸŒ Network Considerations:")
    print("   â€¢ Increase delay for rate-limited websites")
    print("   â€¢ Reduce max_concurrent for unstable connections")
    print("   â€¢ Use conservative config for shared hosting")

if __name__ == "__main__":
    print("ğŸ  Property Crawler - Optimized Version")
    print("Press Ctrl+C to stop at any time")
    
    show_usage_tips()
    print()
    
    asyncio.run(main())